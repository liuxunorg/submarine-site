{
  
  
  

    "/development/contribution/how_to_contribute_code.html": {
      "title": "Contributing to Apache Submarine (Code)",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Contributing to Apache Submarine ( Code )NOTE : Apache Submarine is an Apache2 License Software.Any contributions to Zeppelin (Source code, Documents, Image, Website) means you agree with license all your contributions as Apache2 License.Setting upHere are some tools you will need to build and test Zeppelin.Software Configuration Management ( SCM )Since Zeppelin uses Git for it&amp;#39;s SCM system, you need git client installed in your development machine.Integrated Development Environment ( IDE )You are free to use whatever IDE you prefer, or your favorite command line editor.Build ToolsTo build the code, installOracle Java 7Apache MavenGetting the source codeFirst of all, you need Zeppelin source code. The official location of Zeppelin is http://git.apache.org/zeppelin.git.git accessGet the source code on your development machine using git.git clone git://git.apache.org/zeppelin.git zeppelinYou may also want to develop against a specific branch. For example, for branch-0.5.6git clone -b branch-0.5.6 git://git.apache.org/zeppelin.git zeppelinApache Submarine follows Fork &amp;amp; Pull as a source control workflow.If you want to not only build Zeppelin but also make any changes, then you need to fork Zeppelin github mirror repository and make a pull request.Before making a pull request, please take a look Contribution Guidelines.Buildmvn installTo skip testmvn install -DskipTestsTo build with specific spark / hadoop versionmvn install -Dspark.version=x.x.x -Dhadoop.version=x.x.xFor the further Run Zeppelin server in development modeOption 1 - Command LineCopy the conf/zeppelin-site.xml.template to zeppelin-server/src/main/resources/zeppelin-site.xml and change the configurations in this file if requiredRun the following commandcd zeppelin-serverHADOOP_HOME=YOUR_HADOOP_HOME JAVA_HOME=YOUR_JAVA_HOME mvn exec:java -Dexec.mainClass=&amp;quot;org.apache.zeppelin.server.ZeppelinServer&amp;quot; -Dexec.args=&amp;quot;&amp;quot;Option 2 - Daemon ScriptNote: Make sure you first run mvn clean install -DskipTestsin your zeppelin root directory, otherwise your server build will fail to find the required dependencies in the local repro.or use daemon scriptbin/zeppelin-daemon startServer will be run on http://localhost:8080.Option 3 - IDECopy the conf/zeppelin-site.xml.template to zeppelin-server/src/main/resources/zeppelin-site.xml and change the configurations in this file if requiredZeppelinServer.java Main classGenerating Thrift CodeSome portions of the Zeppelin code are generated by Thrift. For most Zeppelin changes, you don&amp;#39;t need to worry about this. But if you modify any of the Thrift IDL files (e.g. zeppelin-interpreter/src/main/thrift/*.thrift), then you also need to regenerate these files and submit their updated version as part of your patch.To regenerate the code, install thrift-0.9.2 and then run the following command to generate thrift code.cd &amp;lt;zeppelin_home&amp;gt;/zeppelin-interpreter/src/main/thrift./genthrift.shRun Selenium testZeppelin has set of integration tests using Selenium. To run these test, first build and run Zeppelin and make sure Zeppelin is running on port 8080. Then you can run test using following commandTEST_SELENIUM=true mvn test -Dtest=[TEST_NAME] -DfailIfNoTests=false -pl &amp;#39;zeppelin-interpreter,zeppelin-zengine,zeppelin-server&amp;#39;For example, to run ParagraphActionIT,TEST_SELENIUM=true mvn test -Dtest=ParagraphActionsIT -DfailIfNoTests=false -pl &amp;#39;zeppelin-interpreter,zeppelin-zengine,zeppelin-server&amp;#39;You&amp;#39;ll need Firefox web browser installed in your development environment. While CI server uses Firefox 31.0 to run selenium test, it is good idea to install the same version (disable auto update to keep the version).Where to StartYou can find issues for beginner &amp;amp; newbieStay involvedContributors should join the Zeppelin mailing lists.dev@submarine.apache.org is for people who want to contribute code to Zeppelin. subscribe, unsubscribe, archivesIf you have any issues, create a ticket in JIRA.",
      "url": " /development/contribution/how_to_contribute_code.html",
      "group": "development/contribution",
      "excerpt": "How can you contribute to Apache Submarine project? This document covers from setting up your develop environment to making a pull request on Github."
    }
    ,
    
  

    "/development/contribution/how_to_contribute_website.html": {
      "title": "Contributing to Apache Submarine (Website)",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Contributing to Apache Submarine ( Website )This page will give you an overview of how to build and contribute to the documentation of Apache Submarine.The online documentation at submarine.apache.org is also generated from the files found here.NOTE : Apache Submarine is an Apache2 License Software.Any contributions to Zeppelin (Source code, Documents, Image, Website) means you agree with license all your contributions as Apache2 License.Getting the source codeFirst of all, you need Zeppelin source code. The official location of Zeppelin is http://git.apache.org/zeppelin.git.Documentation website is hosted in &amp;#39;master&amp;#39; branch under /docs/ dir.git accessFirst of all, you need the website source code. The official location of mirror for Zeppelin is http://git.apache.org/zeppelin.git.Get the source code on your development machine using git.git clone git://git.apache.org/zeppelin.gitcd docsApache Submarine follows Fork &amp;amp; Pull as a source control workflow.If you want to not only build Zeppelin but also make any changes, then you need to fork Zeppelin github mirror repository and make a pull request.BuildYou&amp;#39;ll need to install some prerequisites to build the code. Please check Build documentation section in docs/README.md.Run website in development modeWhile you&amp;#39;re modifying website, you might want to see preview of it. Please check Run website section in docs/README.md.Then you&amp;#39;ll be able to access it on http://localhost:4000 with your web browser.Making a Pull RequestWhen you are ready, just make a pull-request.Alternative wayYou can directly edit .md files in /docs/ directory at the web interface of github and make pull-request immediately.Stay involvedContributors should join the Zeppelin mailing lists.dev@submarine.apache.org is for people who want to contribute code to Zeppelin. subscribe, unsubscribe, archivesIf you have any issues, create a ticket in JIRA.",
      "url": " /development/contribution/how_to_contribute_website.html",
      "group": "development/contribution",
      "excerpt": "How can you contribute to Apache Submarine project website? This document covers from building Zeppelin documentation site to making a pull request on Github."
    }
    ,
    
  

    "/development/contribution/useful_developer_tools.html": {
      "title": "Useful Developer Tools",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Useful Developer ToolsDeveloping zeppelin-webCheck zeppelin-web: Local Development.ToolsSVM: Scala Version Managersvm would be useful when changing scala version frequently.JDK change script: OSXthis script would be helpful when changing JDK version frequently.function setjdk() {  if [ $# -ne 0 ]; then  # written based on OSX.   # use diffrent base path for other OS  removeFromPath &amp;#39;/System/Library/Frameworks/JavaVM.framework/Home/bin&amp;#39;  if [ -n &amp;quot;${JAVA_HOME+x}&amp;quot; ]; then    removeFromPath $JAVA_HOME  fi  export JAVA_HOME=`/usr/libexec/java_home -v $@`  export PATH=$JAVA_HOME/bin:$PATH  fi}function removeFromPath() {  export PATH=$(echo $PATH | sed -E -e &amp;quot;s;:$1;;&amp;quot; -e &amp;quot;s;$1:?;;&amp;quot;)}you can use this function like setjdk 1.8 / setjdk 1.7Building Submodules Selectively# build `zeppelin-web` onlymvn clean -pl &amp;#39;zeppelin-web&amp;#39; package -DskipTests;# build `zeppelin-server` and its dependencies onlymvn clean package -pl &amp;#39;spark,spark-dependencies,python,markdown,zeppelin-server&amp;#39; --am -DskipTests# build spark related modules with default profiles: scala 2.10 mvn clean package -pl &amp;#39;spark,spark-dependencies,zeppelin-server&amp;#39; --am -DskipTests# build spark related modules with profiles: scala 2.11, spark 2.1 hadoop 2.7 ./dev/change_scala_version.sh 2.11mvn clean package -Pspark-2.1 -Phadoop-2.7 -Pscala-2.11 -pl &amp;#39;spark,spark-dependencies,zeppelin-server&amp;#39; --am -DskipTests# build `zeppelin-server` and `markdown` with dependenciesmvn clean package -pl &amp;#39;markdown,zeppelin-server&amp;#39; --am -DskipTestsRunning Individual Tests# run the `HeliumBundleFactoryTest` test classmvn test -pl &amp;#39;zeppelin-server&amp;#39; --am -DfailIfNoTests=false -Dtest=HeliumBundleFactoryTestRunning Selenium TestsMake sure that Zeppelin instance is started to execute integration tests (= selenium tests).# run the `SparkParagraphIT` test classTEST_SELENIUM=&amp;quot;true&amp;quot; mvn test -pl &amp;#39;zeppelin-server&amp;#39; --am -DfailIfNoTests=false -Dtest=SparkParagraphIT# run the `testSqlSpark` test function only in the `SparkParagraphIT` class# but note that, some test might be dependent on the previous testsTEST_SELENIUM=&amp;quot;true&amp;quot; mvn test -pl &amp;#39;zeppelin-server&amp;#39; --am -DfailIfNoTests=false -Dtest=SparkParagraphIT#testSqlSpark",
      "url": " /development/contribution/useful_developer_tools.html",
      "group": "development/contribution",
      "excerpt": ""
    }
    ,
    
  

    "/development/writing_zeppelin_interpreter.html": {
      "title": "Writing a New Interpreter",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Writing a New InterpreterWhat is Apache Submarine InterpreterApache Submarine Interpreter is a language backend. For example to use scala code in Zeppelin, you need a scala interpreter.Every Interpreters belongs to an InterpreterGroup.Interpreters in the same InterpreterGroup can reference each other. For example, SparkSqlInterpreter can reference SparkInterpreter to get SparkContext from it while they&amp;#39;re in the same group.InterpreterSetting is configuration of a given InterpreterGroup and a unit of start/stop interpreter.All Interpreters in the same InterpreterSetting are launched in a single, separate JVM process. The Interpreter communicates with Zeppelin engine via Thrift.In &amp;#39;Separate Interpreter(scoped / isolated) for each note&amp;#39; mode which you can see at the Interpreter Setting menu when you create a new interpreter, new interpreter instance will be created per note. But it still runs on the same JVM while they&amp;#39;re in the same InterpreterSettings.Make your own InterpreterCreating a new interpreter is quite simple. Just extend org.apache.zeppelin.interpreter abstract class and implement some methods.For your interpreter project, you need to make interpreter-parent as your parent project and use plugin maven-enforcer-plugin, maven-dependency-plugin and maven-resources-plugin. Here&amp;#39;s one sample pom.xml &amp;lt;project xmlns=&amp;quot;http://maven.apache.org/POM/4.0.0&amp;quot; xmlns:xsi=&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot; xsi:schemaLocation=&amp;quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&amp;quot;&amp;gt;    &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;    &amp;lt;parent&amp;gt;        &amp;lt;artifactId&amp;gt;interpreter-parent&amp;lt;/artifactId&amp;gt;        &amp;lt;groupId&amp;gt;org.apache.zeppelin&amp;lt;/groupId&amp;gt;        &amp;lt;version&amp;gt;0.8.0-SNAPSHOT&amp;lt;/version&amp;gt;        &amp;lt;relativePath&amp;gt;../interpreter-parent&amp;lt;/relativePath&amp;gt;    &amp;lt;/parent&amp;gt;    ...    &amp;lt;dependencies&amp;gt;        &amp;lt;dependency&amp;gt;            &amp;lt;groupId&amp;gt;org.apache.zeppelin&amp;lt;/groupId&amp;gt;            &amp;lt;artifactId&amp;gt;zeppelin-interpreter&amp;lt;/artifactId&amp;gt;            &amp;lt;version&amp;gt;${project.version}&amp;lt;/version&amp;gt;            &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;        &amp;lt;/dependency&amp;gt;    &amp;lt;/dependencies&amp;gt;    &amp;lt;build&amp;gt;        &amp;lt;plugins&amp;gt;            &amp;lt;plugin&amp;gt;                &amp;lt;artifactId&amp;gt;maven-enforcer-plugin&amp;lt;/artifactId&amp;gt;            &amp;lt;/plugin&amp;gt;            &amp;lt;plugin&amp;gt;                &amp;lt;artifactId&amp;gt;maven-dependency-plugin&amp;lt;/artifactId&amp;gt;            &amp;lt;/plugin&amp;gt;            &amp;lt;plugin&amp;gt;                &amp;lt;artifactId&amp;gt;maven-resources-plugin&amp;lt;/artifactId&amp;gt;            &amp;lt;/plugin&amp;gt;        &amp;lt;/plugins&amp;gt;    &amp;lt;/build&amp;gt;&amp;lt;/project&amp;gt;You should include org.apache.zeppelin:zeppelin-interpreter:[VERSION] as your interpreter&amp;#39;s dependency in pom.xml. BesAnd you should put your jars under your interpreter directory with a specific directory name. Zeppelin server reads interpreter directories recursively and initializes interpreters including your own interpreter.There are three locations where you can store your interpreter group, name and other information. Zeppelin server tries to find the location below. Next, Zeppelin tries to find interpreter-setting.json in your interpreter jar.{ZEPPELIN_INTERPRETER_DIR}/{YOUR_OWN_INTERPRETER_DIR}/interpreter-setting.jsonHere is an example of interpreter-setting.json on your own interpreter.[  {    &amp;quot;group&amp;quot;: &amp;quot;your-group&amp;quot;,    &amp;quot;name&amp;quot;: &amp;quot;your-name&amp;quot;,    &amp;quot;className&amp;quot;: &amp;quot;your.own.interpreter.class&amp;quot;,    &amp;quot;properties&amp;quot;: {      &amp;quot;properties1&amp;quot;: {        &amp;quot;envName&amp;quot;: null,        &amp;quot;propertyName&amp;quot;: &amp;quot;property.1.name&amp;quot;,        &amp;quot;defaultValue&amp;quot;: &amp;quot;propertyDefaultValue&amp;quot;,        &amp;quot;description&amp;quot;: &amp;quot;Property description&amp;quot;,        &amp;quot;type&amp;quot;: &amp;quot;textarea&amp;quot;      },      &amp;quot;properties2&amp;quot;: {        &amp;quot;envName&amp;quot;: PROPERTIES_2,        &amp;quot;propertyName&amp;quot;: null,        &amp;quot;defaultValue&amp;quot;: &amp;quot;property2DefaultValue&amp;quot;,        &amp;quot;description&amp;quot;: &amp;quot;Property 2 description&amp;quot;,        &amp;quot;type&amp;quot;: &amp;quot;textarea&amp;quot;      }, ...    },    &amp;quot;editor&amp;quot;: {      &amp;quot;language&amp;quot;: &amp;quot;your-syntax-highlight-language&amp;quot;,      &amp;quot;editOnDblClick&amp;quot;: false,      &amp;quot;completionKey&amp;quot;: &amp;quot;TAB&amp;quot;    }  },  {    ...  }]Finally, Zeppelin uses static initialization with the following:static {  Interpreter.register(&amp;quot;MyInterpreterName&amp;quot;, MyClassName.class.getName());}Static initialization is deprecated and will be supported until 0.6.0.The name will appear later in the interpreter name option box during the interpreter configuration process.The name of the interpreter is what you later write to identify a paragraph which should be interpreted using this interpreter.%MyInterpreterNamesome interpreter specific code...Editor setting for InterpreterYou can add editor object to interpreter-setting.json file to specify paragraph editor settings.LanguageIf the interpreter uses a specific programming language (like Scala, Python, SQL), it is generally recommended to add a syntax highlighting supported for that to the note paragraph editor.To check out the list of languages supported, see the mode-*.js files under zeppelin-web/bower_components/ace-builds/src-noconflict or from github.com/ajaxorg/ace-builds.If you want to add a new set of syntax highlighting,  Add the mode-*.js file to zeppelin-web/bower.json (when built, zeppelin-web/src/index.html will be changed automatically).Add language field to editor object. Note that if you don&amp;#39;t specify language field, your interpreter will use plain text mode for syntax highlighting. Let&amp;#39;s say you want to set your language to java, then add:&amp;quot;editor&amp;quot;: {  &amp;quot;language&amp;quot;: &amp;quot;java&amp;quot;}Edit on double clickIf your interpreter uses mark-up language such as markdown or HTML, set editOnDblClick to true so that text editor opens on pargraph double click and closes on paragraph run. Otherwise set it to false.&amp;quot;editor&amp;quot;: {  &amp;quot;editOnDblClick&amp;quot;: false}Completion key (Optional)By default, Ctrl+dot(.) brings autocompletion list in the editor.Through completionKey, each interpreter can configure autocompletion key.Currently TAB is only available option.&amp;quot;editor&amp;quot;: {  &amp;quot;completionKey&amp;quot;: &amp;quot;TAB&amp;quot;}Install your interpreter binaryOnce you have built your interpreter, you can place it under the interpreter directory with all its dependencies.[ZEPPELIN_HOME]/interpreter/[INTERPRETER_NAME]/Configure your interpreterTo configure your interpreter you need to follow these steps:Add your interpreter class name to the zeppelin.interpreters property in conf/zeppelin-site.xml.Property value is comma separated [INTERPRETER_CLASS_NAME].For example,&amp;lt;property&amp;gt;&amp;lt;name&amp;gt;zeppelin.interpreters&amp;lt;/name&amp;gt;&amp;lt;value&amp;gt;org.apache.zeppelin.spark.SparkInterpreter,org.apache.zeppelin.spark.PySparkInterpreter,org.apache.zeppelin.spark.SparkSqlInterpreter,org.apache.zeppelin.spark.DepInterpreter,org.apache.zeppelin.markdown.Markdown,org.apache.zeppelin.shell.ShellInterpreter,org.apache.zeppelin.hive.HiveInterpreter,com.me.MyNewInterpreter&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;Add your interpreter to the default configuration which is used when there is no zeppelin-site.xml.Start Zeppelin by running ./bin/zeppelin-daemon.sh start.In the interpreter page, click the +Create button and configure your interpreter properties.Now you are done and ready to use your interpreter.Note : Interpreters released with zeppelin have a default configuration which is used when there is no conf/zeppelin-site.xml.Use your interpreter0.5.0Inside of a note, %[INTERPRETER_NAME] directive will call your interpreter.Note that the first interpreter configuration in zeppelin.interpreters will be the default one.For example,%myintpval a = &amp;quot;My interpreter&amp;quot;println(a)0.6.0 and laterInside of a note, %[INTERPRETER_GROUP].[INTERPRETER_NAME] directive will call your interpreter.You can omit either [INTERPRETER_GROUP] or [INTERPRETER_NAME]. If you omit [INTERPRETER_NAME], then first available interpreter will be selected in the [INTERPRETER_GROUP].Likewise, if you skip [INTERPRETER_GROUP], then [INTERPRETER_NAME] will be chosen from default interpreter group.For example, if you have two interpreter myintp1 and myintp2 in group mygrp, you can call myintp1 like%mygrp.myintp1codes for myintp1and you can call myintp2 like%mygrp.myintp2codes for myintp2If you omit your interpreter name, it&amp;#39;ll select first available interpreter in the group ( myintp1 ).%mygrpcodes for myintp1You can only omit your interpreter group when your interpreter group is selected as a default group.%myintp2codes for myintp2ExamplesCheckout some interpreters released with Zeppelin by default.sparkmarkdownshelljdbcContributing a new Interpreter to Zeppelin releasesWe welcome contribution to a new interpreter. Please follow these few steps:First, check out the general contribution guide here.Follow the steps in Make your own Interpreter section and Editor setting for Interpreter above.Add your interpreter as in the Configure your interpreter section above; also add it to the example template zeppelin-site.xml.template.Add tests! They are run by Travis for all changes and it is important that they are self-contained.Include your interpreter as a module in pom.xml.Add documentation on how to use your interpreter under docs/interpreter/. Follow the Markdown style as this example. Make sure you list config settings and provide working examples on using your interpreter in code boxes in Markdown. Link to images as appropriate (images should go to docs/assets/themes/submarine/img/docs-img/). And add a link to your documentation in the navigation menu (docs/_includes/themes/submarine/_navigation.html).Most importantly, ensure licenses of the transitive closure of all dependencies are list in license file.Commit your changes and open a Pull Request on the project Mirror on GitHub; check to make sure Travis CI build is passing.",
      "url": " /development/writing_zeppelin_interpreter.html",
      "group": "development",
      "excerpt": "Apache Submarine Interpreter is a language backend. Every Interpreters belongs to an InterpreterGroup. Interpreters in the same InterpreterGroup can reference each other."
    }
    ,
    
  

    "/ecosystem/overview.html": {
      "title": "Install",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Submarine Quick Start GuidePrerequisiteMust:Apache Hadoop version newer than 2.7.3Optional:Enable YARN ServiceEnable GPU on YARNEnable Docker on YARNBuild Docker imagesSubmarine ConfigurationAfter submarine 0.2.0, it supports two runtimes which are YARN service runtime and Linkedin&amp;#39;s TonY runtime for YARN. Each runtime can support both Tensorflow and PyTorch framework. But we don&amp;#39;t need to worry about the  usage because the two runtime implements the same interface.So before we start running a job, the runtime type should be picked. Theruntime choice may vary depending on different requirements. Check belowtable to choose your runtime.Note that if you want to quickly try Submarine on new or existing YARN cluster, use TonY runtime will help you get start easier(0.2.0+)Hadoop (YARN) VersionIs Docker EnabledIs GPU EnabledAcceptable Runtime2.7.3 ~ 3.0.XXXTonY3.1.0+XXTonY3.1.0+X√TonY3.1.0+√XTonY/YARN-Service3.1.0+√√TonY/YARN-ServiceFor the environment setup, please check InstallationGuide or InstallationGuideCNOnce the applicable runtime is chosen and environment is ready, a submarine.xml need to be created under $HADOOP_CONF_DIR. To use the TonY runtime, please set below value in the submarine configuration.Configuration NameDescriptionsubmarine.runtime.class&amp;quot;org.apache.hadoop.yarn.submarine.runtimes.tony.TonyRuntimeFactory&amp;quot; or &amp;quot;org.apache.hadoop.yarn.submarine.runtimes.yarnservice.YarnServiceRuntimeFactory&amp;quot;A sample submarine.xml is here:&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;&amp;lt;configuration&amp;gt;  &amp;lt;property&amp;gt;    &amp;lt;name&amp;gt;submarine.runtime.class&amp;lt;/name&amp;gt;    &amp;lt;value&amp;gt;org.apache.hadoop.yarn.submarine.runtimes.tony.TonyRuntimeFactory&amp;lt;/value&amp;gt;    &amp;lt;!-- Alternatively, you can use:    &amp;lt;value&amp;gt;org.apache.hadoop.yarn.submarine.runtimes.yarnservice.YarnServiceRuntimeFactory&amp;lt;/value&amp;gt;    --&amp;gt;  &amp;lt;/property&amp;gt;&amp;lt;/configuration&amp;gt;For more Submarine configuration:Configuration NameDescriptionsubmarine.localization.max-allowed-file-size-mbOptional. This sets a size limit to the file/directory to be localized in &amp;quot;-localization&amp;quot; CLI option. 2GB by default.Launch Training JobThis section will give us an idea of how the submarine CLI looks like.Although the run job command looks simple, different job may have very different parameters.For a quick try on Mnist example with TonY runtime, check TonY Mnist ExampleFor a quick try on Cifar10 example with YARN native service runtime, check YARN Service Cifar10 ExampleGet job history / logsGet Job Status from CLICLASSPATH=path-to/hadoop-conf:path-to/hadoop-submarine-all-${SUBMARINE_VERSION}-hadoop-${HADOOP_VERSION}.jar java org.apache.hadoop.yarn.submarine.client.cli.Cli job show --name tf-job-001Output looks like:Job Meta Info:  Application Id: application_1532131617202_0005  Input Path: hdfs://default/dataset/cifar-10-data  Checkpoint Path: hdfs://default/tmp/cifar-10-jobdir  Run Parameters: --name tf-job-001 --docker_image &amp;lt;your-docker-image&amp;gt;                  (... all your commandline before run the job)After that, you can run tensorboard --logdir=&amp;lt;checkpoint-path&amp;gt; to view Tensorboard of the job.Get component logs from a training jobWe can use yarn logs -applicationId &amp;lt;applicationId&amp;gt; to get logs from CLI.Or from YARN UI:Submarine Commandline optionsusage: ... job run -framework &amp;lt;arg&amp;gt;             Framework to use.                              Valid values are: tensorflow, pytorch.                              The default framework is Tensorflow. -checkpoint_path &amp;lt;arg&amp;gt;       Training output directory of the job, could                              be local or other FS directory. This                              typically includes checkpoint files and                              exported model -docker_image &amp;lt;arg&amp;gt;          Docker image name/tag -env &amp;lt;arg&amp;gt;                   Common environment variable of worker/ps -input_path &amp;lt;arg&amp;gt;            Input of the job, could be local or other FS                              directory -name &amp;lt;arg&amp;gt;                  Name of the job -num_ps &amp;lt;arg&amp;gt;                Number of PS tasks of the job, by default                              it is 0 -num_workers &amp;lt;arg&amp;gt;           Numnber of worker tasks of the job, by                              default it is 1 -ps_docker_image &amp;lt;arg&amp;gt;       Specify docker image for PS, when this is                              not specified, PS uses --docker_image as                              default. -ps_launch_cmd &amp;lt;arg&amp;gt;         Commandline of worker, arguments will be                              directly used to launch the PS -ps_resources &amp;lt;arg&amp;gt;          Resource of each PS, for example                              memory-mb=2048,vcores=2,yarn.io/gpu=2 -queue &amp;lt;arg&amp;gt;                 Name of queue to run the job, by default it                              uses default queue -saved_model_path &amp;lt;arg&amp;gt;      Model exported path (savedmodel) of the job,                              which is needed when exported model is not                              placed under ${checkpoint_path}could be                              local or other FS directory. This will be                              used to serve. -tensorboard &amp;lt;arg&amp;gt;           Should we run TensorBoard for this job? By                              default it is true -verbose                     Print verbose log for troubleshooting -wait_job_finish             Specified when user want to wait the job                              finish -worker_docker_image &amp;lt;arg&amp;gt;   Specify docker image for WORKER, when this                              is not specified, WORKER uses --docker_image                              as default. -worker_launch_cmd &amp;lt;arg&amp;gt;     Commandline of worker, arguments will be                              directly used to launch the worker -worker_resources &amp;lt;arg&amp;gt;      Resource of each worker, for example                              memory-mb=2048,vcores=2,yarn.io/gpu=2 -localization &amp;lt;arg&amp;gt;          Specify localization to remote/local                              file/directory available to all container(Docker).                              Argument format is &amp;quot;RemoteUri:LocalFilePath[:rw]&amp;quot;                              (ro permission is not supported yet).                              The RemoteUri can be a file or directory in local                              or HDFS or s3 or abfs or http .etc.                              The LocalFilePath can be absolute or relative.                              If relative, it&amp;#39;ll be under container&amp;#39;s implied                              working directory.                              This option can be set mutiple times.                              Examples are                              -localization &amp;quot;hdfs:///user/yarn/mydir2:/opt/data&amp;quot;                              -localization &amp;quot;s3a:///a/b/myfile1:./&amp;quot;                              -localization &amp;quot;https:///a/b/myfile2:./myfile&amp;quot;                              -localization &amp;quot;/user/yarn/mydir3:/opt/mydir3&amp;quot;                              -localization &amp;quot;./mydir1:.&amp;quot; -conf &amp;lt;arg&amp;gt;                  User specified configuration, as                              key=val pairs.Notes:When using localization option to make a collection of dependency Pythonscripts available to entry python script in the container, you may also need toset PYTHONPATH environment variable as below to avoid module import errorreported from entry_script.py.... job run  # the entry point  --localization entry_script.py:&amp;lt;path&amp;gt;/entry_script.py  # the dependency Python scripts of the entry point  --localization other_scripts_dir:&amp;lt;path&amp;gt;/other_scripts_dir  # the PYTHONPATH env to make dependency available to entry script  --env PYTHONPATH=&amp;quot;&amp;lt;path&amp;gt;/other_scripts_dir&amp;quot;  --worker_launch_cmd &amp;quot;python &amp;lt;path&amp;gt;/entry_script.py ...&amp;quot;Build From SourceIf you want to build the Submarine project by yourself, you can follow it here",
      "url": " /ecosystem/overview.html",
      "group": "quickstart",
      "excerpt": "This page will help you get started and will guide you through installing Apache Submarine and running it in the command line."
    }
    ,
    
  

    "/ecosystem/zeppelin.html": {
      "title": "Install",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Submarine Integration Zeppelin 中文版Hadoop Submarine is the latest machine learning framework subproject in the Hadoop 3.1 release. It allows Hadoop to support Tensorflow, MXNet,Caffe, Spark, etc. A variety of deep learning frameworks provide a full-featured system framework for machine learning algorithm development, distributed model training, model management, and model publishing, combined with hadoop&amp;#39;s intrinsic data storage and data processing capabilities to enable data scientists to Good mining and the value of the data.A deep learning algorithm project requires data acquisition, data processing, data cleaning, interactive visual programming adjustment parameters, algorithm testing, algorithm publishing, algorithm job scheduling, offline model training, model online services and many other processes and processes. Zeppelin is a web-based notebook that supports interactive data analysis. You can use SQL, Scala, Python, etc. to make data-driven, interactive, collaborative documents.You can use the more than 20 interpreters in zeppelin (for example: spark, hive, Cassandra, Elasticsearch, Kylin, HBase, etc.) to collect data, clean data, feature extraction, etc. in the data in Hadoop before completing the machine learning model training. The data preprocessing process.By integrating submarine in zeppelin, we use zeppelin&amp;#39;s data discovery, data analysis and data visualization and collaboration capabilities to visualize the results of algorithm development and parameter adjustment during machine learning model training.ArchitectureAs shown in the figure above, how the Submarine develops and models the machine learning algorithms through Zeppelin is explained from the system architecture.After installing and deploying Hadoop 3.1+ and Zeppelin, submarine will create a fully separate Zeppelin Submarine interpreter Docker container for each user in YARN. This container contains the development and runtime environment for Tensorflow. Zeppelin Server connects to the Zeppelin Submarine interpreter Docker container in YARN. allows algorithmic engineers to perform algorithm development and data visualization in Tensorflow&amp;#39;s stand-alone environment in Zeppelin Notebook.After the algorithm is developed, the algorithm engineer can submit the algorithm directly to the YARN in offline transfer training in Zeppelin, real-time demonstration of model training with Submarine&amp;#39;s TensorBoard for each algorithm engineer.You can not only complete the model training of the algorithm, but you can also use the more than twenty interpreters in Zeppelin. Complete the data preprocessing of the model, For example, you can perform data extraction, filtering, and feature extraction through the Spark interpreter in Zeppelin in the Algorithm Note.In the future, you can also use Zeppelin&amp;#39;s upcoming Workflow workflow orchestration service. You can complete Spark, Hive data processing and Tensorflow model training in one Note. It is organized into a workflow through visualization, etc., and the scheduling of jobs is performed in the production environment.Zeppelin Submarine interpreterAs shown in the figure above, from the internal implementation, how Submarine combines Zeppelin&amp;#39;s machine learning algorithm development and model training.The algorithm engineer created a Tensorflow notebook (left image) in Zeppelin by using Submarine interpreter.It is important to note that you need to complete the development of the entire algorithm in a Note.You can use Spark for data preprocessing in some of the paragraphs in Note.Use Python for algorithm development and debugging of Tensorflow in other paragraphs of notebook, Submarine creates a Zeppelin Submarine Interpreter Docker Container for you in YARN, which contains the following features and services:Shell Command line tool：Allows you to view the system environment in the Zeppelin Submarine Interpreter Docker Container, Install the extension tools you need or the Python dependencies.Kerberos lib：Allows you to perform kerberos authentication and access to Hadoop clusters with Kerberos authentication enabled.Tensorflow environment：Allows you to develop tensorflow algorithm code.Python environment：Allows you to develop tensorflow code.Complete a complete algorithm development with a Note in Zeppelin. If this algorithm contains multiple modules, You can write different algorithm modules in multiple paragraphs in Note. The title of each paragraph is the name of the algorithm module. The content of the paragraph is the code content of this algorithm module.HDFS Client：Zeppelin Submarine Interpreter will automatically submit the algorithm code you wrote in Note to HDFS.Submarine interpreter Docker Image It is Submarine that provides you with an image file that supports Tensorflow (CPU and GPU versions).And installed the algorithm library commonly used by Python.You can also install other development dependencies you need on top of the base image provided by Submarine.When you complete the development of the algorithm module, You can do this by creating a new paragraph in Note and typing %submarine dashboard. Zeppelin will create a Submarine Dashboard. The machine learning algorithm written in this Note can be submitted to YARN as a JOB by selecting the JOB RUN command option in the Control Panel. Create a Tensorflow Model Training Docker Container, The container contains the following sections:Tensorflow environmentHDFS Client Will automatically download the algorithm file Mount from HDFS into the container for distributed model training. Mount the algorithm file to the Work Dir path of the container.Submarine Tensorflow Docker Image There is Submarine that provides you with an image file that supports Tensorflow (CPU and GPU versions). And installed the algorithm library commonly used by Python. You can also install other development dependencies you need on top of the base image provided by Submarine.Submarine shell（Optional）After creating a Note with Submarine Interpreter in Zeppelin, You can add a paragraph to Note if you need it. Using the %submarine.sh identifier, you can use the Shell command to perform various operations on the Submarine Interpreter Docker Container, such as:View the Pythone version in the ContainerView the system environment of the ContainerInstall the dependencies you need yourselfKerberos certification with kinitUse Hadoop in Container for HDFS operations, etc.Submarine pythonYou can add one or more paragraphs to Note. Write the algorithm module for Tensorflow in Python using the %submarine.python identifier.Submarine DashboardAfter writing the Tensorflow algorithm by using %submarine.python, You can add a paragraph to Note. Enter the %submarine dashboard and execute it. Zeppelin will create a Submarine Dashboard.With Submarine Dashboard you can do all the operational control of Submarine, for example:Usage：Display Submarine&amp;#39;s command description to help developers locate problems.Refresh：Zeppelin will erase all your input in the Dashboard.Tensorboard：You will be redirected to the Tensorboard WEB system created by Submarine for each user. With Tensorboard you can view the real-time status of the Tensorflow model training in real time.CommandJOB RUN：Selecting JOB RUN will display the parameter input interface for submitting JOB.NameDescriptionCheckpoint PathSubmarine sets up a separate Checkpoint path for each user&amp;#39;s Note for Tensorflow training. Saved the training data for this Note history, Used to train the output of model data, Tensorboard uses the data in this path for model presentation. Users cannot modify it. For example: hdfs://cluster1/... , The environment variable name for Checkpoint Path is %checkpoint_path%, You can use %checkpoint_path% instead of the input value in Data Path in PS Launch Cmd and Worker Launch Cmd.Input PathThe user specifies the data data directory of the Tensorflow algorithm. Only HDFS-enabled directories are supported. The environment variable name for Data Path is %input_path%, You can use %input_path% instead of the input value in Data Path in PS Launch Cmd and Worker Launch Cmd.PS Launch CmdTensorflow Parameter services launch command，eg.：python cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --num-gpus=0 ...Worker Launch CmdTensorflow Worker services launch command，eg.：python cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --num-gpus=1 ...JOB STOPYou can choose to execute the JOB STOP command. Stop a Tensorflow model training task that has been submitted and is runningTENSORBOARD STARTYou can choose to execute the TENSORBOARD START command to create your TENSORBOARD Docker Container.TENSORBOARD STOPYou can choose to execute the TENSORBOARD STOP command to stop and destroy your TENSORBOARD Docker Container.Run Command：Execute the action command of your choiceClean Chechkpoint：Checking this option will clear the data in this Note&amp;#39;s Checkpoint Path before each JOB RUN execution.Submarine interpreter AttributesZeppelin Submarine interpreter provides the following properties to customize the Submarine interpreterAttribute nameAttribute valueDescriptionDOCKER_CONTAINER_TIME_ZONESet the time zone in the containerDOCKER_HADOOP_HDFS_HOME/hadoop-3.1-0Hadoop path in the following 3 images（SUBMARINEINTERPRETERDOCKER_IMAGE、tf.parameter.services.docker.image、tf.worker.services.docker.image）DOCKER_JAVA_HOME/opt/javaJAVA path in the following 3 images（SUBMARINEINTERPRETERDOCKER_IMAGE、tf.parameter.services.docker.image、tf.worker.services.docker.image）HADOOP_YARN_SUBMARINE_JARPath to the Submarine JAR package in the Hadoop-3.1+ release installed on the Zeppelin serverINTERPRETER_LAUNCH_MODElocal/yarnRun the Submarine interpreter instance in local or YARN local mainly for submarine interpreter development and debugging YARN mode for production environmentSUBMARINE_HADOOP_CONF_DIRSet the HADOOP-CONF path to support multiple Hadoop cluster environmentsSUBMARINE_HADOOP_HOMEHadoop-3.1+ above path installed on the Zeppelin serverSUBMARINE_HADOOP_KEYTABKeytab file path for a hadoop cluster with kerberos authentication turned onSUBMARINE_HADOOP_PRINCIPALPRINCIPAL information for the keytab file of the hadoop cluster with kerberos authentication turned onSUBMARINE_INTERPRETER_DOCKER_IMAGEAt INTERPRETERLAUNCHMODE=yarn, Submarine uses this image to create a Zeppelin Submarine interpreter container to create an algorithm development environment for the user.docker.container.networkYARN&amp;#39;s Docker network namemachinelearing.distributed.enableWhether to use the model training of the distributed mode JOB RUN submissionshell.command.timeout.millisecs60000Execute timeout settings for shell commands in the Submarine interpreter containersubmarine.algorithm.hdfs.pathSave machine-based algorithms developed using Submarine interpreter to HDFS as filessubmarine.yarn.queueroot.defaultSubmarine submits model training YARN queue nametf.checkpoint.pathTensorflow checkpoint path, Each user will create a user&amp;#39;s checkpoint secondary path using the username under this path. Each algorithm submitted by the user will create a checkpoint three-level path using the note id (the user&amp;#39;s Tensorboard uses the checkpoint data in this path for visual display)tf.parameter.services.cpuNumber of CPU cores applied to Tensorflow parameter services when Submarine submits model distributed trainingtf.parameter.services.docker.imageSubmarine creates a mirror for Tensorflow parameter services when submitting model distributed trainingtf.parameter.services.gpuGPU cores applied to Tensorflow parameter services when Submarine submits model distributed trainingtf.parameter.services.memory2GMemory resources requested by Tensorflow parameter services when Submarine submits model distributed trainingtf.parameter.services.numNumber of Tensorflow parameter services used by Submarine to submit model distributed trainingtf.tensorboard.enabletrueCreate a separate Tensorboard for each usertf.worker.services.cpuSubmarine submits model resources for Tensorflow worker services when submitting model trainingtf.worker.services.docker.imageSubmarine creates a mirror for Tensorflow worker services when submitting model distributed trainingtf.worker.services.gpuSubmarine submits GPU resources for Tensorflow worker services when submitting model trainingtf.worker.services.memorySubmarine submits model resources for Tensorflow worker services when submitting model trainingtf.worker.services.numNumber of Tensorflow worker services used by Submarine to submit model distributed trainingyarn.webapp.http.addresshttp://hadoop:8088YARN web ui addresszeppelin.interpreter.rpc.portRange29914You need to export this port in the SUBMARINEINTERPRETERDOCKER_IMAGE configuration image. RPC communication for Zeppelin Server and Submarine interpreter containerszeppelin.ipython.grpc.message_size33554432Message size setting for IPython grpc in Submarine interpreter containerzeppelin.ipython.launch.timeout30000IPython execution timeout setting in Submarine interpreter containerzeppelin.pythonpythonExecution path of python in Submarine interpreter containerzeppelin.python.maxResult10000The maximum number of python execution results returned from the Submarine interpreter containerzeppelin.python.useIPythonfalseIPython is currently not supported and must be falsezeppelin.submarine.auth.typesimple/kerberosHas Hadoop turned on kerberos authentication?More introductionYoutube Submarine Channel：https://www.youtube.com/channel/UC4JBt8Y8VJ0BW0IM9YpdCyQ",
      "url": " /ecosystem/zeppelin.html",
      "group": "quickstart",
      "excerpt": "This page will help you get started and will guide you through installing Apache Submarine and running it in the command line."
    }
    ,
    
  

    "/ecosystem/zeppelin_cn.html": {
      "title": "Install",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Submarine Integration Zeppelin English versionHadoop Submarine 是 Hadoop 3.1 版本中最新的机器学习框架子项目。 它允许 Hadoop 支持 Tensorflow，PyTroch，MXNet，Caffe 等各种深度学习框架，进行机器学习算法开发，分布式模型训练，模型管理和模型发布提供了全功能的系统框架，并结合了hadoop的内在数据存储和数据处理功能使数据科学家能够更好地挖掘和获取数据的价值。深度学习算法项目需要数据采集，数据处理，数据清理，交互式可视化编程调整参数，算法测试，算法发布，算法作业调度，离线模型训练，模型在线服务等很多处理过程。 Zeppelin 是一款基于 WEB 的 Notebook 支持交互式数据分析。 您可以使用 SQL，Scala，Python 等语法进行交互式的数据开发工作。您可以使用 zeppelin 中 20 多种解释器（例如：spark，hive，Cassandra，Elasticsearch，Kylin，HBase等）对 Hadoop 中的数据进行数据提取，数据清理，特征提取等数据预处理过程。通过将 Zeppelin 集成到 Submarine 中，我们可以使用 zeppelin 的数据发现，数据分析和数据可视化以及协作功能，完成机器学习模型训练期间的可视化算法开发和参数调整工作。Architecture如上图所示，从系统架构上阐述了 Submarine 如何通过 Zeppelin 进行机器学习算法的开发和模型训练。在安装部署完 Hadoop 3.1+ 和 Zeppelin 后，Submarine 将为每个用户在 YARN 中创建一个完全独立的 Zeppelin Submarine interpreter Docker 容器，这个容器中包含了 Tensorflow 的开发和运行环境，Zeppelin Server 通过连接 YARN 中的 Zeppelin Submarine interpreter Docker 容器，让算法工程师可以在 Zeppelin Notebook 中进行 Tensorflow 的单机环境下的算法开发和数据可视化展现。在完成算法开发后，算法工程师可以直接在 Zeppelin 中将算法以 JOB 的方式提交到 YARN 中进行模型的离线分布式训练，通过 Submarine 为每个算法工程师提供的 TensorBoard 进行模型训练的实时展示。你不仅仅可以完成算法的模型训练，你还可以借助 Zeppelin 中原有的二十多种解释器，完成模型的数据预处理工作，例如：你可以在算法 Note 中通过 Zeppelin 中的 Spark 解释器进行数据抽取、过滤和特征提取等操作。未来，你还可以借助 Zeppelin 即将发布的 Workflow 工作流编排服务，你可以在一个 Note 中完成 Spark、Hive 的数据处理和 Tensorflow 模型训练，通过可视化等方式编排成一个工作流，在生产环境中进行作业的周期性调度。Zeppelin Submarine interpreter如上图所示，从内部实现上阐述了 Submarine 如何结合 Zeppelin 进行机器学习算法的开发和模型训练。算法工程师在 Zeppelin 中通过使用 Submarine interpreter 创建一个 Tensorflow 的 notebook（左图）。需要注意的是你需要在一个 Note 中完成整个算法的开发。你可以在 Note 中的一些段落中使用例如 Spark 进行数据预处理工作。在 notebook 的其他段落中使用 Python 进行 Tensorflow 的算法开发和调试，Submarine 会在 YARN 中为你创建 Zeppelin Submarine Interpreter Docker Container，容器中包含了以下功能和服务：Shell 命令行工具：让你可以查看 Zeppelin Submarine Interpreter Docker Container 中的系统环境，安装自己需要的扩展工具或是 Python 依赖库。Kerberos 库：让你可以进行 kerberos 认证，访问启用了 Kerberos 认证的 Hadoop 集群。Tensorflow 运行环境：让你可以进行 tensorflow 算法代码的开发。Python 运行环境：让你可以进行 tensorflow 代码的开发。在 Zeppelin 中通过一个 Note 来完成一个完整的算法开发，如果这个算法包含了多个模块，你可以在 Note 中通过多个段落分别编写不同的算法模块，每个段落的标题就是算法模块的名称，段落的内容就是这个算法模块的代码内容。HDFS Client：Zeppelin Submarine Interpreter 会自动将你在 Note 中编写的算法代码提交到 HDFS 中。Submarine interpreter Docker Image 是由 Submarine 为你提供支持 Tensorflow （CPU 和 GPU 版本）的镜像文件，并安装了 Python 常用的算法库，你也可以在 Submarine 提供的基础镜像之上安装你所需要的其他开发依赖库进行使用。当你完成算法模块的开发后，你可以在 Note 中新建一个段落输入 %submarine dashboard 后执行， Zeppelin 将会创建出 Submarine Dashboard ，在控制面板中通过选择 JOB RUN 命令选项就可以将这个 Note 中编写的机器学习算法作为一个 JOB 提交到 YARN 中，创建出 Tensorflow Model Training Docker Container ，容器中包含了以下部分：Tensorflow 运行环境。HDFS Client 会自动完成从 HDFS 中下载算法文件 Mount 到容器中进行分布式模型训练，并将算法文件 Mount 到容器的 Work Dir 路径。Submarine Tensorflow Docker Image 是有 Submarine 为你提供支持 Tensorflow （CPU 和 GPU 版本）的镜像文件，并安装了 Python 常用的算法库，你也可以在 Submarine 提供的基础镜像之上安装你所需要的其他开发依赖库进行使用。Submarine shell（Optional）在 Zeppelin 中使用 Submarine Interpreter 创建 Note 之后，如果有需要你可以在 Note 中新增段落，使用 %submarine.sh 标识符，你可以使用 Shell 命令对  Submarine Interpreter Docker Container 进行各种操作，例如：查看 Container 中的 Pythone 版本查看 Container 的系统环境安装你自己需要的依赖库通过 kinit 进行 kerberos 认证使用 Container 中的 Hadoop 进行 HDFS 操作等等Submarine python你可以在 Note 中新增一个或多个段落，使用 %submarine.python 标识符，使用 Python 编写 Tensorflow 的算法模块。Submarine Dashboard在通过使用 %submarine.python 编写完 Tensorflow 算法后，你可以在 Note 中新增一个段落，输入 %submarine dashboard 并执行，Zeppelin 将会创建出 Submarine 的 Dashboard。通过 Submarine Dashboard 你可以完成对 Submarine 的所有操作控制，例如：Usage：显示 Submarine 的命令说明，帮助开发人员进行问题定位。Refresh：Zeppelin 将会清除你在 Dashboard 中的所有输入内容。Tensorboard：将会跳转到 Submarine 为每个用户创建的 Tensorboard WEB 系统，通过 Tensorboard 你可以实时查看 Tensorflow 模型训练的实时状态。CommandJOB RUN：选择 JOB RUN 将会显示提交 JOB 的参数输入界面参数名称说明Checkpoint PathSubmarine 为每个用户的每个 Note 设置了一个单独的 Tensorflow 训练的 Checkpoint 路径，保存了这个 Note 历史的训练数据，用于训练模型数据的输出，Tensorboard 使用这个路径中的数据进行模型展示，用户不可修改。例如：hdfs://cluster1/... ，Checkpoint Path 的环境变量名称是 %checkpoint_path%，你可以在 PS Launch Cmd 和 Worker Launch Cmd 中直接使用 %checkpoint_path% 代替在 Checkpoint Path 中的输入值Data Path用户指定 Tensorflow 算法的数据数据目录，只支持 HDFS 的目录。Data Path 的环境变量名称是 %input_path%，你可以在 PS Launch Cmd 和 Worker Launch Cmd 中直接使用 %input_path% 代替在 Data Path 中的输入值PS Launch CmdTensorflow Parameter services launch command，例如：python cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --num-gpus=0 ...Worker Launch CmdTensorflow Worker services launch command，例如：python cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --num-gpus=1 ...JOB STOP你可以选择执行 JOB STOP 命令，停止一个已经提交和正在运行的 Tensorflow 模型训练任务TENSORBOARD START你可以选择执行 TENSORBOARD START 命令，创建你的 TENSORBOARD Docker Container。TENSORBOARD STOP你可以选择执行 TENSORBOARD STOP 命令，停止并销毁你的 TENSORBOARD Docker Container。Run Command：执行你所选择的操作命令Clean Chechkpoint：勾选上这个选项，在每次执行 JOB RUN 之前，将会清除这个 Note 的 Checkpoint Path 中的数据。Submarine interpreter AttributesZeppelin Submarine interpreter 提供了以下属性对 Submarine 解释器进行定制属性名属性值说明DOCKER_HADOOP_HDFS_HOME例如：/hadoop-3.1在以下3个镜像中的 Hadoop 路径（SUBMARINEINTERPRETERDOCKER_IMAGE、tf.parameter.services.docker.image、tf.worker.services.docker.image）DOCKER_JAVA_HOME例如：/opt/java在以下3个镜像中的 JAVA 路径（SUBMARINEINTERPRETERDOCKER_IMAGE、tf.parameter.services.docker.image、tf.worker.services.docker.image）HADOOP_YARN_SUBMARINE_JAR在 Zeppelin 服务器中安装的 Hadoop-3.1+ 版本中的 Submarine JAR 包的路径INTERPRETER_LAUNCH_MODElocal/yarn将 Submarine 解释器实例运行在 local 或 YARN 中 local 主要用于 submarine interpreter 的开发和调试 YARN 模式用于生产环境SUBMARINE_HADOOP_CONF_DIR设置 HADOOP-CONF 路径，可以用于支持多个hadoop 集群环境的情况SUBMARINE_HADOOP_HOME在 Zeppelin 服务器中安装的 Hadoop-3.1+ 以上版本路径SUBMARINE_HADOOP_KEYTAB用于开启了 kerberos 认证的 hadoop 集群的 keytab 文件路径SUBMARINE_HADOOP_PRINCIPAL用于开启了 kerberos 认证的 hadoop 集群的 keytab 文件的 PRINCIPAL 信息SUBMARINE_INTERPRETER_DOCKER_IMAGE在 INTERPRETERLAUNCHMODE=yarn 时，Submarine 会使用这个镜像创建 Zeppelin Submarine interpreter 容器为用户创建算法开发环境docker.container.networkYARN 的 Docker 网络名称machinelearing.distributed.enable是否使用分布式模式 JOB RUN 提交的模型训练shell.command.timeout.millisecs60000在 Submarine interpreter 容器中执行 Shell 命令的超时设置submarine.algorithm.hdfs.path将使用 Submarine interpreter 开发的机器学习算法以文件的方式保存到 HDFS 中submarine.yarn.queueroot.defaultSubmarine 提交模型训练的 YARN 队列名称tf.checkpoint.pathTensorflow checkpoint 路径，每个用户都会在这个路径下使用用户名创建用户的 checkpoint 二级路径，用户提交的每个算法都会使用note id 创建checkpoint 三级路径（用户的 Tensorboard 使用这个路径里的 checkpoint 数据进行可视化展示）tf.parameter.services.cpuSubmarine 提交模型分布式训练时为 Tensorflow parameter services 申请的 CPU 核数tf.parameter.services.docker.imageSubmarine 提交模型分布式训练时创建 Tensorflow parameter services 使用的镜像tf.parameter.services.gpuSubmarine 提交模型分布式训练时为 Tensorflow parameter services 申请的 GPU 核数tf.parameter.services.memory2GSubmarine 提交模型分布式训练时为 Tensorflow parameter services 申请的 Memory 资源tf.parameter.services.numSubmarine 提交模型分布式训练时使用的 Tensorflow parameter services 数目tf.tensorboard.enabletrue为每个用户创建一个独立的 Tensorboardtf.worker.services.cpuSubmarine 提交模型训练时为 Tensorflow worker services 申请的 CPU 资源tf.worker.services.docker.imageSubmarine 提交模型分布式训练时创建 Tensorflow worker services 使用的镜像tf.worker.services.gpuSubmarine 提交模型训练时为 Tensorflow worker services 申请的 GPU 资源tf.worker.services.memorySubmarine 提交模型训练时为 Tensorflow worker services 申请的 CPU 资源tf.worker.services.numSubmarine 提交模型分布式训练时使用的 Tensorflow worker services 数目yarn.webapp.http.address例如：http://hadoop-3-1:8088YARN web ui addresszeppelin.interpreter.rpc.portRange29914需要在 SUBMARINEINTERPRETERDOCKER_IMAGE 配置的镜像中 export 这个端口，用于 Zeppelin Server 和 Submarine interpreter 容器进行 RPC 通讯zeppelin.ipython.grpc.message_size33554432Submarine interpreter 容器中 IPython grpc 的消息大小设置zeppelin.ipython.launch.timeout30000Submarine interpreter 容器中 IPython 执行超时设置zeppelin.pythonpythonSubmarine interpreter 容器中 python 的执行路径zeppelin.python.maxResult10000从 Submarine interpreter 容器中返回 python 执行结果的最大条数zeppelin.python.useIPythonfalse目前不支持IPython，必须为falsezeppelin.submarine.auth.typesimple/kerberosHadoop 是否开启了 kerberos 认证更多介绍Youtube Submarine Channel：https://www.youtube.com/channel/UC4JBt8Y8VJ0BW0IM9YpdCyQ",
      "url": " /ecosystem/zeppelin_cn.html",
      "group": "quickstart",
      "excerpt": "This page will help you get started and will guide you through installing Apache Submarine and running it in the command line."
    }
    ,
    
  
  
  

    "/quickstart/index.html": {
      "title": "Install",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Submarine Quick Start GuidePrerequisiteMust:Apache Hadoop version newer than 2.7.3Optional:Enable YARN ServiceEnable GPU on YARNEnable Docker on YARNBuild Docker imagesSubmarine ConfigurationAfter submarine 0.2.0, it supports two runtimes which are YARN service runtime and Linkedin&amp;#39;s TonY runtime for YARN. Each runtime can support both Tensorflow and PyTorch framework. But we don&amp;#39;t need to worry about the  usage because the two runtime implements the same interface.So before we start running a job, the runtime type should be picked. Theruntime choice may vary depending on different requirements. Check belowtable to choose your runtime.Note that if you want to quickly try Submarine on new or existing YARN cluster, use TonY runtime will help you get start easier(0.2.0+)Hadoop (YARN) VersionIs Docker EnabledIs GPU EnabledAcceptable Runtime2.7.3 ~ 3.0.XXXTonY3.1.0+XXTonY3.1.0+X√TonY3.1.0+√XTonY/YARN-Service3.1.0+√√TonY/YARN-ServiceFor the environment setup, please check InstallationGuide or InstallationGuideCNOnce the applicable runtime is chosen and environment is ready, a submarine.xml need to be created under $HADOOP_CONF_DIR. To use the TonY runtime, please set below value in the submarine configuration.Configuration NameDescriptionsubmarine.runtime.class&amp;quot;org.apache.hadoop.yarn.submarine.runtimes.tony.TonyRuntimeFactory&amp;quot; or &amp;quot;org.apache.hadoop.yarn.submarine.runtimes.yarnservice.YarnServiceRuntimeFactory&amp;quot;A sample submarine.xml is here:&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;&amp;lt;configuration&amp;gt;  &amp;lt;property&amp;gt;    &amp;lt;name&amp;gt;submarine.runtime.class&amp;lt;/name&amp;gt;    &amp;lt;value&amp;gt;org.apache.hadoop.yarn.submarine.runtimes.tony.TonyRuntimeFactory&amp;lt;/value&amp;gt;    &amp;lt;!-- Alternatively, you can use:    &amp;lt;value&amp;gt;org.apache.hadoop.yarn.submarine.runtimes.yarnservice.YarnServiceRuntimeFactory&amp;lt;/value&amp;gt;    --&amp;gt;  &amp;lt;/property&amp;gt;&amp;lt;/configuration&amp;gt;For more Submarine configuration:Configuration NameDescriptionsubmarine.localization.max-allowed-file-size-mbOptional. This sets a size limit to the file/directory to be localized in &amp;quot;-localization&amp;quot; CLI option. 2GB by default.Launch Training JobThis section will give us an idea of how the submarine CLI looks like.Although the run job command looks simple, different job may have very different parameters.For a quick try on Mnist example with TonY runtime, check TonY Mnist ExampleFor a quick try on Cifar10 example with YARN native service runtime, check YARN Service Cifar10 ExampleGet job history / logsGet Job Status from CLICLASSPATH=path-to/hadoop-conf:path-to/hadoop-submarine-all-${SUBMARINE_VERSION}-hadoop-${HADOOP_VERSION}.jar java org.apache.hadoop.yarn.submarine.client.cli.Cli job show --name tf-job-001Output looks like:Job Meta Info:  Application Id: application_1532131617202_0005  Input Path: hdfs://default/dataset/cifar-10-data  Checkpoint Path: hdfs://default/tmp/cifar-10-jobdir  Run Parameters: --name tf-job-001 --docker_image &amp;lt;your-docker-image&amp;gt;                  (... all your commandline before run the job)After that, you can run tensorboard --logdir=&amp;lt;checkpoint-path&amp;gt; to view Tensorboard of the job.Get component logs from a training jobWe can use yarn logs -applicationId &amp;lt;applicationId&amp;gt; to get logs from CLI.Or from YARN UI:Submarine Commandline optionsusage: ... job run -framework &amp;lt;arg&amp;gt;             Framework to use.                              Valid values are: tensorflow, pytorch.                              The default framework is Tensorflow. -checkpoint_path &amp;lt;arg&amp;gt;       Training output directory of the job, could                              be local or other FS directory. This                              typically includes checkpoint files and                              exported model -docker_image &amp;lt;arg&amp;gt;          Docker image name/tag -env &amp;lt;arg&amp;gt;                   Common environment variable of worker/ps -input_path &amp;lt;arg&amp;gt;            Input of the job, could be local or other FS                              directory -name &amp;lt;arg&amp;gt;                  Name of the job -num_ps &amp;lt;arg&amp;gt;                Number of PS tasks of the job, by default                              it is 0 -num_workers &amp;lt;arg&amp;gt;           Numnber of worker tasks of the job, by                              default it is 1 -ps_docker_image &amp;lt;arg&amp;gt;       Specify docker image for PS, when this is                              not specified, PS uses --docker_image as                              default. -ps_launch_cmd &amp;lt;arg&amp;gt;         Commandline of worker, arguments will be                              directly used to launch the PS -ps_resources &amp;lt;arg&amp;gt;          Resource of each PS, for example                              memory-mb=2048,vcores=2,yarn.io/gpu=2 -queue &amp;lt;arg&amp;gt;                 Name of queue to run the job, by default it                              uses default queue -saved_model_path &amp;lt;arg&amp;gt;      Model exported path (savedmodel) of the job,                              which is needed when exported model is not                              placed under ${checkpoint_path}could be                              local or other FS directory. This will be                              used to serve. -tensorboard &amp;lt;arg&amp;gt;           Should we run TensorBoard for this job? By                              default it is true -verbose                     Print verbose log for troubleshooting -wait_job_finish             Specified when user want to wait the job                              finish -worker_docker_image &amp;lt;arg&amp;gt;   Specify docker image for WORKER, when this                              is not specified, WORKER uses --docker_image                              as default. -worker_launch_cmd &amp;lt;arg&amp;gt;     Commandline of worker, arguments will be                              directly used to launch the worker -worker_resources &amp;lt;arg&amp;gt;      Resource of each worker, for example                              memory-mb=2048,vcores=2,yarn.io/gpu=2 -localization &amp;lt;arg&amp;gt;          Specify localization to remote/local                              file/directory available to all container(Docker).                              Argument format is &amp;quot;RemoteUri:LocalFilePath[:rw]&amp;quot;                              (ro permission is not supported yet).                              The RemoteUri can be a file or directory in local                              or HDFS or s3 or abfs or http .etc.                              The LocalFilePath can be absolute or relative.                              If relative, it&amp;#39;ll be under container&amp;#39;s implied                              working directory.                              This option can be set mutiple times.                              Examples are                              -localization &amp;quot;hdfs:///user/yarn/mydir2:/opt/data&amp;quot;                              -localization &amp;quot;s3a:///a/b/myfile1:./&amp;quot;                              -localization &amp;quot;https:///a/b/myfile2:./myfile&amp;quot;                              -localization &amp;quot;/user/yarn/mydir3:/opt/mydir3&amp;quot;                              -localization &amp;quot;./mydir1:.&amp;quot; -conf &amp;lt;arg&amp;gt;                  User specified configuration, as                              key=val pairs.Notes:When using localization option to make a collection of dependency Pythonscripts available to entry python script in the container, you may also need toset PYTHONPATH environment variable as below to avoid module import errorreported from entry_script.py.... job run  # the entry point  --localization entry_script.py:&amp;lt;path&amp;gt;/entry_script.py  # the dependency Python scripts of the entry point  --localization other_scripts_dir:&amp;lt;path&amp;gt;/other_scripts_dir  # the PYTHONPATH env to make dependency available to entry script  --env PYTHONPATH=&amp;quot;&amp;lt;path&amp;gt;/other_scripts_dir&amp;quot;  --worker_launch_cmd &amp;quot;python &amp;lt;path&amp;gt;/entry_script.py ...&amp;quot;Build From SourceIf you want to build the Submarine project by yourself, you can follow it here",
      "url": " /quickstart/index.html",
      "group": "quickstart",
      "excerpt": "This page will help you get started and will guide you through installing Apache Submarine and running it in the command line."
    }
    ,
    
  

    "/quickstart/install.html": {
      "title": "Install",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Submarine Installation GuidePrerequisites(Please note that all following prerequisites are just an example for you to install. You can always choose to install your own version of kernel, different users, different drivers, etc.).Operating SystemThe operating system and kernel versions we have tested are as shown in the following table, which is the recommneded minimum required versions.EnviromentVerionOperating Systemcentos-release-7-3.1611.el7.centos.x86_64Kernal3.10.0-514.el7.x86_64User &amp;amp; GroupAs there are some specific users and groups recommended to be created to install hadoop/docker. Please create them if they are missing.adduser hdfsadduser mapredadduser yarnaddgroup hadoopusermod -aG hdfs,hadoop hdfsusermod -aG mapred,hadoop mapredusermod -aG yarn,hadoop yarnusermod -aG hdfs,hadoop hadoopgroupadd dockerusermod -aG docker yarnusermod -aG docker hadoopGCC VersionCheck the version of GCC tool (to compile kernel).gcc --versiongcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)# install if neededyum install gcc make g++Kernel header &amp;amp; Kernel devel# Approach 1：yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r)# Approach 2：wget http://vault.centos.org/7.3.1611/os/x86_64/Packages/kernel-headers-3.10.0-514.el7.x86_64.rpmrpm -ivh kernel-headers-3.10.0-514.el7.x86_64.rpmGPU Servers (Only for Nvidia GPU equipped nodes)lspci | grep -i nvidia# If the server has gpus, you can get info like this：04:00.0 3D controller: NVIDIA Corporation Device 1b38 (rev a1)82:00.0 3D controller: NVIDIA Corporation Device 1b38 (rev a1)Nvidia Driver Installation (Only for Nvidia GPU equipped nodes)To make a clean installation, if you have requirements to upgrade GPU drivers. If nvidia driver/cuda has been installed before, They should be uninstalled firstly.# uninstall cuda：sudo /usr/local/cuda-10.0/bin/uninstall_cuda_10.0.pl# uninstall nvidia-driver：sudo /usr/bin/nvidia-uninstallTo check GPU version, install nvidia-detectyum install nvidia-detect# run &amp;#39;nvidia-detect -v&amp;#39; to get reqired nvidia driver version：nvidia-detect -vProbing for supported NVIDIA devices...[10de:13bb] NVIDIA Corporation GM107GL [Quadro K620]This device requires the current xyz.nm NVIDIA driver kmod-nvidia[8086:1912] Intel Corporation HD Graphics 530An Intel display controller was also detectedPay attention to This device requires the current xyz.nm NVIDIA driver kmod-nvidia.Download the installer like NVIDIA-Linux-x86_64-390.87.run.Some preparatory work for nvidia driver installation. (This is follow normal Nvidia GPU driver installation, just put here for your convenience)# It may take a while to updateyum -y updateyum -y install kernel-develyum -y install epel-releaseyum -y install dkms# Disable nouveauvim /etc/default/grub# Add the following configuration in “GRUB_CMDLINE_LINUX” partrd.driver.blacklist=nouveau nouveau.modeset=0# Generate configurationgrub2-mkconfig -o /boot/grub2/grub.cfgvim /etc/modprobe.d/blacklist.conf# Add confiuration:blacklist nouveaumv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r)-nouveau.imgdracut /boot/initramfs-$(uname -r).img $(uname -r)rebootCheck whether nouveau is disabledlsmod | grep nouveau  # return null# install nvidia driversh NVIDIA-Linux-x86_64-390.87.runSome options during the installationInstall NVIDIA&amp;#39;s 32-bit compatibility libraries (Yes)centos Install NVIDIA&amp;#39;s 32-bit compatibility libraries (Yes)Would you like to run the nvidia-xconfig utility to automatically update your X configuration file... (NO)Check nvidia driver installationnvidia-smiReference：https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.htmlDocker InstallationWe recommend to use Docker version &amp;gt;= 1.12.5, following steps are just for your reference. You can always to choose other approaches to install Docker.yum -y updateyum -y install yum-utilsyum-config-manager --add-repo https://yum.dockerproject.org/repo/main/centos/7yum -y update# Show available packagesyum search --showduplicates docker-engine# Install docker 1.12.5yum -y --nogpgcheck install docker-engine-1.12.5*systemctl start dockerchown hadoop:netease /var/run/docker.sockchown hadoop:netease /usr/bin/dockerReference：https://docs.docker.com/cs-engine/1.12/Docker ConfigurationAdd a file, named daemon.json, under the path of /etc/docker/. Please replace the variables of imageregistryip, etcdhostip, localhostip, yarndnsregistryhostip, dnshost_ip with specific ips according to your environments.{    &amp;quot;insecure-registries&amp;quot;: [&amp;quot;${image_registry_ip}:5000&amp;quot;],    &amp;quot;cluster-store&amp;quot;:&amp;quot;etcd://${etcd_host_ip1}:2379,${etcd_host_ip2}:2379,${etcd_host_ip3}:2379&amp;quot;,    &amp;quot;cluster-advertise&amp;quot;:&amp;quot;{localhost_ip}:2375&amp;quot;,    &amp;quot;dns&amp;quot;: [&amp;quot;${yarn_dns_registry_host_ip}&amp;quot;, &amp;quot;${dns_host_ip1}&amp;quot;],    &amp;quot;hosts&amp;quot;: [&amp;quot;tcp://{localhost_ip}:2375&amp;quot;, &amp;quot;unix:///var/run/docker.sock&amp;quot;]}Restart docker daemon：sudo systemctl restart dockerDocker EE version$ docker versionClient: Version:      1.12.5 API version:  1.24 Go version:   go1.6.4 Git commit:   7392c3b Built:        Fri Dec 16 02:23:59 2016 OS/Arch:      linux/amd64Server: Version:      1.12.5 API version:  1.24 Go version:   go1.6.4 Git commit:   7392c3b Built:        Fri Dec 16 02:23:59 2016 OS/Arch:      linux/amd64Nvidia-docker Installation (Only for Nvidia GPU equipped nodes)Submarine depends on nvidia-docker 1.0 versionwget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.1/nvidia-docker-1.0.1-1.x86_64.rpmsudo rpm -i /tmp/nvidia-docker*.rpm# Start nvidia-dockersudo systemctl start nvidia-docker# Check nvidia-docker status：systemctl status nvidia-docker# Check nvidia-docker log：journalctl -u nvidia-docker# Test nvidia-docker-plugincurl http://localhost:3476/v1.0/docker/cliAccording to nvidia-driver version, add folders under the path of  /var/lib/nvidia-docker/volumes/nvidia_driver/mkdir /var/lib/nvidia-docker/volumes/nvidia_driver/390.87# 390.8 is nvidia driver versionmkdir /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/binmkdir /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/lib64cp /usr/bin/nvidia* /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/bincp /usr/lib64/libcuda* /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/lib64cp /usr/lib64/libnvidia* /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/lib64# Test with nvidia-sminvidia-docker run --rm nvidia/cuda:9.0-devel nvidia-smiTest docker, nvidia-docker, nvidia-driver installation# Test 1nvidia-docker run -rm nvidia/cuda nvidia-smi# Test 2nvidia-docker run -it tensorflow/tensorflow:1.9.0-gpu bash# In docker containerpythonimport tensorflow as tftf.test.is_gpu_available()The way to uninstall nvidia-docker 1.0)Reference:https://github.com/NVIDIA/nvidia-docker/tree/1.0Tensorflow ImageThere is no need to install CUDNN and CUDA on the servers, because CUDNN and CUDA can be added in the docker images. we can get basic docker images by following WriteDockerfile.md.The basic Dockerfile doesn&amp;#39;t support kerberos security. if you need kerberos, you can get write a Dockerfile like thisFROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04# Pick up some TF dependenciesRUN apt-get update &amp;amp;&amp;amp; apt-get install -y --allow-downgrades --no-install-recommends         build-essential         cuda-command-line-tools-9-0         cuda-cublas-9-0         cuda-cufft-9-0         cuda-curand-9-0         cuda-cusolver-9-0         cuda-cusparse-9-0         curl         libcudnn7=7.0.5.15-1+cuda9.0         libfreetype6-dev         libpng12-dev         libzmq3-dev         pkg-config         python         python-dev         rsync         software-properties-common         unzip         &amp;amp;&amp;amp;     apt-get clean &amp;amp;&amp;amp;     rm -rf /var/lib/apt/lists/*RUN export DEBIAN_FRONTEND=noninteractive &amp;amp;&amp;amp; apt-get update &amp;amp;&amp;amp; apt-get install -yq krb5-user libpam-krb5 &amp;amp;&amp;amp; apt-get cleanRUN curl -O https://bootstrap.pypa.io/get-pip.py &amp;amp;&amp;amp;     python get-pip.py &amp;amp;&amp;amp;     rm get-pip.pyRUN pip --no-cache-dir install         Pillow         h5py         ipykernel         jupyter         matplotlib         numpy         pandas         scipy         sklearn         &amp;amp;&amp;amp;     python -m ipykernel.kernelspec# Install TensorFlow GPU version.RUN pip --no-cache-dir install     http://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.8.0-cp27-none-linux_x86_64.whlRUN apt-get update &amp;amp;&amp;amp; apt-get install git -yRUN apt-get update &amp;amp;&amp;amp; apt-get install -y openjdk-8-jdk wget# Downloadhadoop-3.1.1.tar.gzRUN wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-3.1.1/hadoop-3.1.1.tar.gzRUN tar zxf hadoop-3.1.1.tar.gzRUN mv hadoop-3.1.1 hadoop-3.1.0# Download jdk which supports kerberosRUN wget -qO jdk8.tar.gz &amp;#39;http://${kerberos_jdk_url}/jdk-8u152-linux-x64.tar.gz&amp;#39;RUN tar xzf jdk8.tar.gz -C /optRUN mv /opt/jdk* /opt/javaRUN rm jdk8.tar.gzRUN update-alternatives --install /usr/bin/java java /opt/java/bin/java 100RUN update-alternatives --install /usr/bin/javac javac /opt/java/bin/javac 100ENV JAVA_HOME /opt/javaENV PATH $PATH:$JAVA_HOME/binTest tensorflow in a docker containerAfter docker image is built, we can checkTensorflow environments before submitting a yarn job.$ docker run -it ${docker_image_name} /bin/bash# &amp;gt;&amp;gt;&amp;gt; In the docker container$ python$ python &amp;gt;&amp;gt; import tensorflow as tf$ python &amp;gt;&amp;gt; tf.__version__If there are some errors, we could check the following configuration.LDLIBRARYPATH environment variableecho $LD_LIBRARY_PATH/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64The location of libcuda.so.1, libcuda.sols -l /usr/local/nvidia/lib64 | grep libcuda.soEtcd Installationetcd is a distributed reliable key-value store for the most critical data of a distributed system, Registration and discovery of services used in containers.You can also choose alternatives like zookeeper, Consul.To install Etcd on specified servers, we can run Submarine-installer/install.sh$ ./Submarine-installer/install.sh# Etcd statussystemctl status Etcd.serviceCheck Etcd cluster health$ etcdctl cluster-healthmember 3adf2673436aa824 is healthy: got healthy result from http://${etcd_host_ip1}:2379member 85ffe9aafb7745cc is healthy: got healthy result from http://${etcd_host_ip2}:2379member b3d05464c356441a is healthy: got healthy result from http://${etcd_host_ip3}:2379cluster is healthy$ etcdctl member list3adf2673436aa824: name=etcdnode3 peerURLs=http://${etcd_host_ip1}:2380 clientURLs=http://${etcd_host_ip1}:2379 isLeader=false85ffe9aafb7745cc: name=etcdnode2 peerURLs=http://${etcd_host_ip2}:2380 clientURLs=http://${etcd_host_ip2}:2379 isLeader=falseb3d05464c356441a: name=etcdnode1 peerURLs=http://${etcd_host_ip3}:2380 clientURLs=http://${etcd_host_ip3}:2379 isLeader=trueCalico InstallationCalico creates and manages a flat three-tier network, and each container is assigned a routable ip. We just add the steps here for your convenience.You can also choose alternatives like Flannel, OVS.To install Calico on specified servers, we can run Submarine-installer/install.shsystemctl start calico-node.servicesystemctl status calico-node.serviceCheck Calico Network# Run the following command to show the all host status in the cluster except localhost.$ calicoctl node statusCalico process is running.IPv4 BGP status+---------------+-------------------+-------+------------+-------------+| PEER ADDRESS  |     PEER TYPE     | STATE |   SINCE    |    INFO     |+---------------+-------------------+-------+------------+-------------+| ${host_ip1} | node-to-node mesh | up    | 2018-09-21 | Established || ${host_ip2} | node-to-node mesh | up    | 2018-09-21 | Established || ${host_ip3} | node-to-node mesh | up    | 2018-09-21 | Established |+---------------+-------------------+-------+------------+-------------+IPv6 BGP statusNo IPv6 peers found.Create containers to validate calico networkdocker network create --driver calico --ipam-driver calico-ipam calico-networkdocker run --net calico-network --name workload-A -tid busyboxdocker run --net calico-network --name workload-B -tid busyboxdocker exec workload-A ping workload-BHadoop InstallationGet Hadoop ReleaseYou can either get Hadoop release binary or compile from source code. Please follow the https://hadoop.apache.org/ guides.Start yarn serviceYARN_LOGFILE=resourcemanager.log ./sbin/yarn-daemon.sh start resourcemanagerYARN_LOGFILE=nodemanager.log ./sbin/yarn-daemon.sh start nodemanagerYARN_LOGFILE=timeline.log ./sbin/yarn-daemon.sh start timelineserverYARN_LOGFILE=mr-historyserver.log ./sbin/mr-jobhistory-daemon.sh start historyserverStart yarn registery dns servicesudo YARN_LOGFILE=registrydns.log ./yarn-daemon.sh start registrydnsTest with a MR wordcount job./bin/hadoop jar /home/hadoop/hadoop-current/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.0-SNAPSHOT.jar wordcount /tmp/wordcount.txt /tmp/wordcount-output4Tensorflow Job with CPUStandalone ModeClean up apps with the same nameSuppose we want to submit a tensorflow job named standalone-tf, destroy any application with the same name and clean up historical job directories../bin/yarn app -destroy standalone-tf./bin/hdfs dfs -rmr hdfs://${dfs_name_service}/tmp/cifar-10-jobdirwhere ${dfsnameservice} is the hdfs name service you useRun a standalone tensorflow job./bin/yarn jar /home/hadoop/hadoop-current/share/hadoop/yarn/hadoop-yarn-submarine-3.2.0-SNAPSHOT.jar job run  --env DOCKER_JAVA_HOME=/opt/java  --env DOCKER_HADOOP_HDFS_HOME=/hadoop-3.1.0 --name standalone-tf  --docker_image dockerfile-cpu-tf1.8.0-with-models  --input_path hdfs://${dfs_name_service}/tmp/cifar-10-data  --checkpoint_path hdfs://${dfs_name_service}/user/hadoop/tf-checkpoint  --worker_resources memory=4G,vcores=2 --verbose  --worker_launch_cmd &amp;quot;python /test/cifar10_estimator/cifar10_main.py --data-dir=hdfs://${dfs_name_service}/tmp/cifar-10-data --job-dir=hdfs://${dfs_name_service}/tmp/cifar-10-jobdir --train-steps=500 --eval-batch-size=16 --train-batch-size=16 --num-gpus=0&amp;quot;Distributed ModeClean up apps with the same name./bin/yarn app -destroy distributed-tf./bin/hdfs dfs -rmr hdfs://${dfs_name_service}/tmp/cifar-10-jobdirRun a distributed tensorflow job./bin/yarn jar /home/hadoop/hadoop-current/share/hadoop/yarn/hadoop-yarn-submarine-3.2.0-SNAPSHOT.jar job run  --env DOCKER_JAVA_HOME=/opt/java  --env DOCKER_HADOOP_HDFS_HOME=/hadoop-3.1.0 --name distributed-tf  --env YARN_CONTAINER_RUNTIME_DOCKER_CONTAINER_NETWORK=calico-network  --docker_image dockerfile-cpu-tf1.8.0-with-models  --input_path hdfs://${dfs_name_service}/tmp/cifar-10-data  --checkpoint_path hdfs://${dfs_name_service}/user/hadoop/tf-distributed-checkpoint  --worker_resources memory=4G,vcores=2 --verbose  --num_ps 1  --ps_resources memory=4G,vcores=2  --ps_launch_cmd &amp;quot;python /test/cifar10_estimator/cifar10_main.py --data-dir=hdfs://${dfs_name_service}/tmp/cifar-10-data --job-dir=hdfs://${dfs_name_service}/tmp/cifar-10-jobdir --num-gpus=0&amp;quot;  --num_workers 4  --worker_launch_cmd &amp;quot;python /test/cifar10_estimator/cifar10_main.py --data-dir=hdfs://${dfs_name_service}/tmp/cifar-10-data --job-dir=hdfs://${dfs_name_service}/tmp/cifar-10-jobdir --train-steps=500 --eval-batch-size=16 --train-batch-size=16 --sync --num-gpus=0&amp;quot;Tensorflow Job with GPUGPU configurations for both resourcemanager and nodemanagerAdd the yarn resource configuration file, named resource-types.xml   &amp;lt;configuration&amp;gt;     &amp;lt;property&amp;gt;       &amp;lt;name&amp;gt;yarn.resource-types&amp;lt;/name&amp;gt;       &amp;lt;value&amp;gt;yarn.io/gpu&amp;lt;/value&amp;gt;     &amp;lt;/property&amp;gt;   &amp;lt;/configuration&amp;gt;GPU configurations for resourcemanagerThe scheduler used by resourcemanager must be  capacity scheduler, and yarn.scheduler.capacity.resource-calculator in  capacity-scheduler.xml should be DominantResourceCalculator   &amp;lt;configuration&amp;gt;     &amp;lt;property&amp;gt;       &amp;lt;name&amp;gt;yarn.scheduler.capacity.resource-calculator&amp;lt;/name&amp;gt;       &amp;lt;value&amp;gt;org.apache.hadoop.yarn.util.resource.DominantResourceCalculator&amp;lt;/value&amp;gt;     &amp;lt;/property&amp;gt;   &amp;lt;/configuration&amp;gt;GPU configurations for nodemanagerAdd configurations in yarn-site.xml   &amp;lt;configuration&amp;gt;     &amp;lt;property&amp;gt;       &amp;lt;name&amp;gt;yarn.nodemanager.resource-plugins&amp;lt;/name&amp;gt;       &amp;lt;value&amp;gt;yarn.io/gpu&amp;lt;/value&amp;gt;     &amp;lt;/property&amp;gt;   &amp;lt;/configuration&amp;gt;Add configurations in container-executor.cfg   [docker]   ...   # Add configurations in `[docker]` part：   # /usr/bin/nvidia-docker is the path of nvidia-docker command   # nvidia_driver_375.26 means that nvidia driver version is &amp;lt;version&amp;gt;. nvidia-smi command can be used to check the version   docker.allowed.volume-drivers=/usr/bin/nvidia-docker   docker.allowed.devices=/dev/nvidiactl,/dev/nvidia-uvm,/dev/nvidia-uvm-tools,/dev/nvidia1,/dev/nvidia0   docker.allowed.ro-mounts=nvidia_driver_&amp;lt;version&amp;gt;   [gpu]   module.enabled=true   [cgroups]   # /sys/fs/cgroup is the cgroup mount destination   # /hadoop-yarn is the path yarn creates by default   root=/sys/fs/cgroup   yarn-hierarchy=/hadoop-yarn",
      "url": " /quickstart/install.html",
      "group": "quickstart",
      "excerpt": "This page will help you get started and will guide you through installing Apache Submarine and running it in the command line."
    }
    ,
    
  

    "/quickstart/pytorch_with_submarine.html": {
      "title": "SQL with Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;PyTorch support in SubmarineThe following guides explain how to use Apache Submarine that enables you to write in SQL:provides JDBC Interpreter which allows you can connect any JDBC data sources seamlesslyPostgresMySQL MariaDBAWS Redshift Apache HiveApache Phoenix Apache DrillApache Tajoand so on Spark Interpreter supports SparkSQLPython Interpreter supports pandasSQL can create query result including UI widgets using Dynamic Form%sql select age, count(1) value from bank where age &amp;lt; ${maxAge=30} group by age order by ageFor the further information about SQL support in Zeppelin, please check JDBC InterpreterSpark InterpreterPython InterpreterIgniteSQL Interpreter for Apache IgniteKylin Interpreter for Apache Kylin",
      "url": " /quickstart/pytorch_with_submarine.html",
      "group": "quickstart",
      "excerpt": ""
    }
    ,
    
  

    "/quickstart/tensorflow_with_submarine.html": {
      "title": "Spark with Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Tensorflow support in SubmarineFor a brief overview of Apache Spark fundamentals with Apache Submarine, see the following guide:built-in Apache Spark integration.with SparkSQL, PySpark, SparkRinject SparkContext and SQLContext automaticallydependencies loading (jars) at runtime using dependency loader canceling job and displaying its progress supporting Spark Cluster Mode for external spark clusterssupports different context per user / note sharing variables among PySpark, SparkR and Spark through ZeppelinContextLivy Interpreter",
      "url": " /quickstart/tensorflow_with_submarine.html",
      "group": "quickstart",
      "excerpt": ""
    }
    ,
    
  
  
  
  
  

    "/setup/basics/how_to_build.html": {
      "title": "How to Build Zeppelin from source",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}## How to Build Zeppelin from Source#### 0. RequirementsIf you want to build from source, you must first install the following dependencies:      Name    Value        Git    (Any Version)        Maven    3.1.x or higher        JDK    1.8  If you haven&#39;t installed Git and Maven yet, check the [Build requirements](#build-requirements) section and follow the step by step instructions from there.#### 1. Clone the Apache Submarine repository```bashgit clone https://github.com/apache/submarine.git```#### 2. Build sourceYou can build Zeppelin with following maven command:```bashmvn clean package -DskipTests [Options]```If you&#39;re unsure about the options, use the same commands that creates official binary package.```bash# build submarine with version of Apache hadoopmvn clean package -DskipTests -Phadoop-2.9```#### 3. DoneYou can directly start Zeppelin by running after successful build:```bash./bin/zeppelin-daemon.sh start```Check [build-profiles](#build-profiles) section for further build options.If you are behind proxy, follow instructions in [Proxy setting](#proxy-setting-optional) section.If you&#39;re interested in contribution, please check [Contributing to Apache Submarine (Code)](../../development/contribution/how_to_contribute_code.html) and [Contributing to Apache Submarine (Website)](../../development/contribution/how_to_contribute_website.html).### Build profiles#### HadoopTo build with a specific Hadoop version, Hadoop version or specific features, define one or more of the following profiles and options:##### `-Phadoop-[version]`set hadoop major versionAvailable profiles are```-Phadoop-2.7-Phadoop-2.9 (recommend)-Phadoop-3.1-Phadoop-3.2```### Build command examplesHere are some examples with several options:```bash# build with hadoop-2.7mvn clean package -Phadoop-2.7 -DskipTests# build with hadoop-2.9mvn clean package -Phadoop-2.9 -DskipTests# build with hadoop-3.1mvn clean package -Phadoop-3.1 -DskipTests# build with hadoop-3.2mvn clean package -Phadoop-3.2 -DskipTests```## Build requirements### Install requirementsIf you don&#39;t have requirements prepared, install it.(The installation method may vary according to your environment, example is for Ubuntu.)```bashsudo apt-get updatesudo apt-get install gitsudo apt-get install openjdk-8-jdk```### Install maven```bashwget http://www.eu.apache.org/dist/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gzsudo tar -zxf apache-maven-3.3.9-bin.tar.gz -C /usr/local/sudo ln -s /usr/local/apache-maven-3.3.9/bin/mvn /usr/local/bin/mvn```_Notes:_ - Ensure node is installed by running `node --version`   - Ensure maven is running version 3.1.x or higher with `mvn -version` - Configure maven to use more memory than usual by `export MAVEN_OPTS=&quot;-Xmx2g -XX:MaxPermSize=1024m&quot;`## Proxy setting (optional)If you&#39;re behind the proxy, you&#39;ll need to configure maven and npm to pass through it.First of all, configure maven in your `~/.m2/settings.xml`.```xml            proxy-http      true      http      localhost      3128      usr      pwd --&gt;      localhost|127.0.0.1              proxy-https      true      https      localhost      3128      usr      pwd --&gt;      localhost|127.0.0.1      ```Then, next commands will configure npm.```bashnpm config set proxy http://localhost:3128npm config set https-proxy http://localhost:3128npm config set registry &quot;http://registry.npmjs.org/&quot;npm config set strict-ssl false```Configure git as well```bashgit config --global http.proxy http://localhost:3128git config --global https.proxy http://localhost:3128git config --global url.&quot;http://&quot;.insteadOf git://```To clean up, set `active false` in Maven `settings.xml` and run these commands.```bashnpm config rm proxynpm config rm https-proxygit config --global --unset http.proxygit config --global --unset https.proxygit config --global --unset url.&quot;http://&quot;.insteadOf```_Notes:_ - If you are behind NTLM proxy you can use [Cntlm Authentication Proxy](http://cntlm.sourceforge.net/). - Replace `localhost:3128` with the standard pattern `http://user:pwd@host:port`.## PackageTo package the final distribution including the compressed archive, run:```shmvn clean package -Pbuild-distr```To build a distribution with specific profiles, run:```shmvn clean package -Pbuild-distr -Phadoop-2.9```The profiles `-Phadoop-2.9` can be adjusted if you wish to build to a specific spark versions.The archive is generated under _`zeppelin-dist/target`_ directory## Run end-to-end testsZeppelin comes with a set of end-to-end acceptance tests driving headless selenium browser```sh# assumes zeppelin-server running on localhost:8080 (use -Durl=.. to override)mvn verify# or take care of starting/stoping zeppelin-server from packaged zeppelin-distribuion/targetmvn verify -P using-packaged-distr```",
      "url": " /setup/basics/how_to_build.html",
      "group": "setup/basics",
      "excerpt": "How to build Zeppelin from source"
    }
    ,
    
  

    "/setup/deployment/docker.html": {
      "title": "Apache Submarine Releases Docker Images",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Docker Image for Apache Submarine Releases ## Overview This document contains instructions about making docker containers for Zeppelin. It mainly provides guidance into how to create, publish and run docker images for zeppelin releases.## Quick Start### Installing DockerYou need to [install docker](https://docs.docker.com/engine/installation/) on your machine.### Running docker image```bashdocker run -p 8080:8080 --rm --name zeppelin apache/zeppelin: ```* Zeppelin will run at `http://localhost:8080`.If you want to specify `logs` and `notebook` dir, ```bashdocker run -p 8080:8080 --rm -v $PWD/logs:/logs -v $PWD/notebook:/notebook -e ZEPPELIN_LOG_DIR=&#39;/logs&#39; -e ZEPPELIN_NOTEBOOK_DIR=&#39;/notebook&#39; --name zeppelin apache/zeppelin: # e.g &#39;0.7.1&#39;```### Building dockerfile locally```bashcd $ZEPPELIN_HOMEcd scripts/docker/zeppelin/bindocker build -t my-zeppelin:my-tag ./```",
      "url": " /setup/deployment/docker.html",
      "group": "setup/deployment",
      "excerpt": "This document contains instructions about making docker containers for Zeppelin. It mainly provides guidance into how to create, publish and run docker images for zeppelin releases."
    }
    ,
    
  

    "/setup/deployment/yarn_install.html": {
      "title": "Install Zeppelin to connect with existing YARN cluster",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}## IntroductionThis page describes how to pre-configure a bare metal node, configure Zeppelin and connect it to existing YARN cluster running Hortonworks flavour of Hadoop. It also describes steps to configure Spark interpreter of Zeppelin.## Prepare Node### Zeppelin user (Optional)This step is optional, however its nice to run Zeppelin under its own user. In case you do not like to use Zeppelin (hope not) the user could be deleted along with all the packages that were installed for Zeppelin, Zeppelin binary itself and associated directories.Create a zeppelin user and switch to zeppelin user or if zeppelin user is already created then login as zeppelin.```bashuseradd zeppelinsu - zeppelinwhoami```Assuming a zeppelin user is created then running whoami command must return```bashzeppelin```Its assumed in the rest of the document that zeppelin user is indeed created and below installation instructions are performed as zeppelin user.### List of Prerequisites * CentOS 6.x, Mac OSX, Ubuntu 14.X * Java 1.7 * Hadoop client * Spark * Internet connection is required.It&#39;s assumed that the node has CentOS 6.x installed on it. Although any version of Linux distribution should work fine.#### Hadoop clientZeppelin can work with multiple versions &amp; distributions of Hadoop. A complete list is available [here](https://github.com/apache/zeppelin#build). This document assumes Hadoop 2.7.x client libraries including configuration files are installed on Zeppelin node. It also assumes /etc/hadoop/conf contains various Hadoop configuration files. The location of Hadoop configuration files may vary, hence use appropriate location.```bashhadoop versionHadoop 2.7.1.2.3.1.0-2574Subversion git@github.com:hortonworks/hadoop.git -r f66cf95e2e9367a74b0ec88b2df33458b6cff2d0Compiled by jenkins on 2015-07-25T22:36ZCompiled with protoc 2.5.0From source with checksum 54f9bbb4492f92975e84e390599b881dThis command was run using /usr/hdp/2.3.1.0-2574/hadoop/lib/hadoop-common-2.7.1.2.3.1.0-2574.jar```#### SparkSpark is supported out of the box and to take advantage of this, you need to Download appropriate version of Spark binary packages from [Spark Download page](http://spark.apache.org/downloads.html) and unzip it.Zeppelin can work with multiple versions of Spark. A complete list is available [here](https://github.com/apache/zeppelin#build).This document assumes Spark 1.6.0 is installed at /usr/lib/spark.&gt; Note: Spark should be installed on the same node as Zeppelin.&gt; Note: Spark&#39;s pre-built package for CDH 4 doesn&#39;t support yarn.#### ZeppelinCheckout source code from [git://git.apache.org/zeppelin.git](https://github.com/apache/zeppelin.git) or download binary package from [Download page](https://submarine.apache.org/download.html).You can refer [Install](install.html) page for the details.This document assumes that Zeppelin is located under `/home/zeppelin/zeppelin`.## Zeppelin ConfigurationZeppelin configuration needs to be modified to connect to YARN cluster. Create a copy of zeppelin environment shell script.```bashcp /home/zeppelin/zeppelin/conf/zeppelin-env.sh.template /home/zeppelin/zeppelin/conf/zeppelin-env.sh```Set the following properties```bashexport JAVA_HOME=&quot;/usr/java/jdk1.7.0_79&quot;export HADOOP_CONF_DIR=&quot;/etc/hadoop/conf&quot;export ZEPPELIN_JAVA_OPTS=&quot;-Dhdp.version=2.3.1.0-2574&quot;export SPARK_HOME=&quot;/usr/lib/spark&quot;```As /etc/hadoop/conf contains various configurations of YARN cluster, Zeppelin can now submit Spark/Hive jobs on YARN cluster form its web interface. The value of hdp.version is set to 2.3.1.0-2574. This can be obtained by running the following command```bashhdp-select status hadoop-client | sed &#39;s/hadoop-client - (.*)/1/&#39;# It returned  2.3.1.0-2574```## Start/Stop### Start Zeppelin```bashcd /home/zeppelin/zeppelinbin/zeppelin-daemon.sh start```After successful start, visit http://[zeppelin-server-host-name]:8080 with your web browser.### Stop Zeppelin```bashbin/zeppelin-daemon.sh stop```## InterpreterZeppelin provides various distributed processing frameworks to process data that ranges from Spark, JDBC, Ignite and Lens to name a few. This document describes to configure JDBC &amp; Spark interpreters.### HiveZeppelin supports Hive through JDBC interpreter. You might need the information to use Hive and can find in your hive-site.xmlOnce Zeppelin server has started successfully, visit http://[zeppelin-server-host-name]:8080 with your web browser. Click on Interpreter tab next to Notebook dropdown. Look for Hive configurations and set them appropriately. Set them as per Hive installation on YARN cluster.Click on Save button. Once these configurations are updated, Zeppelin will prompt you to restart the interpreter. Accept the prompt and the interpreter will reload the configurations.### SparkIt was assumed that 1.6.0 version of Spark is installed at /usr/lib/spark. Look for Spark configurations and click edit button to add the following properties      Property Name    Property Value    Remarks        master    yarn-client    In yarn-client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN.        spark.driver.extraJavaOptions    -Dhdp.version=2.3.1.0-2574            spark.yarn.am.extraJavaOptions    -Dhdp.version=2.3.1.0-2574      Click on Save button. Once these configurations are updated, Zeppelin will prompt you to restart the interpreter. Accept the prompt and the interpreter will reload the configurations.Spark &amp; Hive notebooks can be written with Zeppelin now. The resulting Spark &amp; Hive jobs will run on configured YARN cluster.## DebugZeppelin does not emit any kind of error messages on web interface when notebook/paragraph is run. If a paragraph fails it only displays ERROR. The reason for failure needs to be looked into log files which is present in logs directory under zeppelin installation base directory. Zeppelin creates a log file for each kind of interpreter.```bash[zeppelin@zeppelin-3529 logs]$ pwd/home/zeppelin/zeppelin/logs[zeppelin@zeppelin-3529 logs]$ ls -ltotal 844-rw-rw-r-- 1 zeppelin zeppelin  14648 Aug  3 14:45 zeppelin-interpreter-hive-zeppelin-zeppelin-3529.log-rw-rw-r-- 1 zeppelin zeppelin 625050 Aug  3 16:05 zeppelin-interpreter-spark-zeppelin-zeppelin-3529.log-rw-rw-r-- 1 zeppelin zeppelin 200394 Aug  3 21:15 zeppelin-zeppelin-zeppelin-3529.log-rw-rw-r-- 1 zeppelin zeppelin  16162 Aug  3 14:03 zeppelin-zeppelin-zeppelin-3529.out[zeppelin@zeppelin-3529 logs]$```",
      "url": " /setup/deployment/yarn_install.html",
      "group": "setup/deployment",
      "excerpt": "This page describes how to pre-configure a bare metal node, configure Apache Submarine and connect it to existing YARN cluster running Hortonworks flavour of Hadoop."
    }
    ,
    
  

    "/setup/operation/configuration.html": {
      "title": "Apache Submarine Configuration",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Apache Submarine Configuration## Zeppelin PropertiesThere are two locations you can configure Apache Submarine.* **Environment variables** can be defined `conf/zeppelin-env.sh`(`confzeppelin-env.cmd` for Windows).* **Java properties** can be defined in `conf/zeppelin-site.xml`.If both are defined, then the **environment variables** will take priority.&gt; Mouse hover on each property and click  then you can get a link for that.      zeppelin-env.sh    zeppelin-site.xml    Default value    Description        ZEPPELIN_PORT    zeppelin.server.port    8080    Zeppelin server port        Note: Please make sure you&#39;re not using the same port with      Zeppelin web application development port (default: 9000).        ZEPPELIN_SSL_PORT    zeppelin.server.ssl.port    8443    Zeppelin Server ssl port (used when ssl environment/property is set to true)        ZEPPELIN_JMX_ENABLE    N/A        Enable JMX by defining &quot;true&quot;        ZEPPELIN_JMX_PORT    N/A    9996    Port number which JMX uses        ZEPPELIN_MEM    N/A    -Xmx1024m -XX:MaxPermSize=512m    JVM mem options        ZEPPELIN_INTP_MEM    N/A    ZEPPELIN_MEM    JVM mem options for interpreter process        ZEPPELIN_JAVA_OPTS    N/A        JVM options  ",
      "url": " /setup/operation/configuration.html",
      "group": "setup/operation",
      "excerpt": "This page will guide you to configure Apache Submarine using either environment variables or Java properties. Also, you can configure SSL for Zeppelin."
    }
    ,
    
  

    "/setup/operation/proxy_setting.html": {
      "title": "Proxy Setting in Apache Submarine",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Proxy Setting ## How to Configure Proxies?Set `http_proxy` and `https_proxy` env variables. (See [more](https://wiki.archlinux.org/index.php/proxy_settings))Currently, Proxy is supported only for these features.- Helium: downloading `helium.json`, installing `npm`, `node`, `yarn`",
      "url": " /setup/operation/proxy_setting.html",
      "group": "security",
      "excerpt": "Apache Submarine supports Helium plugins which fetch required installer packages from remote registry/repositories"
    }
    ,
    
  

    "/setup/operation/trouble_shooting.html": {
      "title": "Trouble Shooting",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Troubleshooting ",
      "url": " /setup/operation/trouble_shooting.html",
      "group": "setup/operation",
      "excerpt": ""
    }
    ,
    
  

    "/setup/operation/upgrading.html": {
      "title": "Manual Zeppelin version upgrade procedure",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Manual upgrade procedure for ZeppelinBasically, newer version of Zeppelin works with previous version notebook directory and configurations.So, copying `notebook` and `conf` directory should be enough.## Instructions1. Stop Zeppelin: `bin/zeppelin-daemon.sh stop`2. Copy your `notebook` and `conf` directory into a backup directory3. Download newer version of Zeppelin and Install. See [Install Guide](../../quickstart/install.html#install).4. Copy backup `notebook` and `conf` directory into newer version of Zeppelin `notebook` and `conf` directory5. Start Zeppelin:  `bin/zeppelin-daemon.sh start`## Migration Guide### Breaking changes in 0.8.xFrom 0.8, Zeppelin has a new type of permission - [Runners](http://submarine.apache.org/docs/0.8.0/setup/security/notebook_authorization.html#authorization-setting)As Runners list is empty in note so everybody can view note although Readers list is not empty. To set all your &quot;writers&quot; to &quot;runners&quot;:1. Copy `notebook` and `conf` directories to 0.8.0,2. Move directory **docs/assets/themes/submarine/note/FixReaders** to new `notebook` directory,3. Start the new Zeppelin and run note **System/Migrate from 0.7**.### Upgrading from Zeppelin 0.7 to 0.8 - From 0.8, we recommend to use `PYSPARK_PYTHON` and `PYSPARK_DRIVER_PYTHON` instead of `zeppelin.pyspark.python` as `zeppelin.pyspark.python` only effects driver. You can use `PYSPARK_PYTHON` and `PYSPARK_DRIVER_PYTHON` as using them in spark. - From 0.8, depending on your device, the keyboard shortcut `Ctrl-L` or `Command-L` which goes to the line somewhere user wants is not supported.  ### Upgrading from Zeppelin 0.6 to 0.7 - From 0.7, we don&#39;t use `ZEPPELIN_JAVA_OPTS` as default value of `ZEPPELIN_INTP_JAVA_OPTS` and also the same for `ZEPPELIN_MEM`/`ZEPPELIN_INTP_MEM`. If user want to configure the jvm opts of interpreter process, please set `ZEPPELIN_INTP_JAVA_OPTS` and `ZEPPELIN_INTP_MEM` explicitly. If you don&#39;t set `ZEPPELIN_INTP_MEM`, Zeppelin will set it to `-Xms1024m -Xmx1024m -XX:MaxPermSize=512m` by default. - Mapping from `%jdbc(prefix)` to `%prefix` is no longer available. Instead, you can use %[interpreter alias] with multiple interpreter setttings on GUI. - Usage of `ZEPPELIN_PORT` is not supported in ssl mode. Instead use `ZEPPELIN_SSL_PORT` to configure the ssl port. Value from `ZEPPELIN_PORT` is used only when `ZEPPELIN_SSL` is set to `false`. - The support on Spark 1.1.x to 1.3.x is deprecated. - From 0.7, we uses `pegdown` as the `markdown.parser.type` option for the `%md` interpreter. Rendered markdown might be different from what you expected - From 0.7 note.json format has been changed to support multiple outputs in a paragraph. Zeppelin will automatically convert old format to new format. 0.6 or lower version can read new note.json format but output will not be displayed. For the detail, see [ZEPPELIN-212](http://issues.apache.org/jira/browse/ZEPPELIN-212) and [pull request](https://github.com/apache/zeppelin/pull/1658). - From 0.7 note storage layer will utilize `GitNotebookRepo` by default instead of `VFSNotebookRepo` storage layer, which is an extension of latter one with versioning capabilities on top of it.",
      "url": " /setup/operation/upgrading.html",
      "group": "setup/operation",
      "excerpt": "This document will guide you through a procedure of manual upgrade your Apache Submarine instance to a newer version. Apache Submarine keeps backward compatibility for the notebook file format."
    }
    ,
    
  
  

    "/usage/interpreter/interpreter_binding_mode.html": {
      "title": "Interpreter Binding Mode",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Interpreter Binding Mode ## OverviewInterpreter Process is a JVM process that communicates with Zeppelin daemon using thrift. Each interpreter process has a single interpreter group, and this interpreter group can have one or more instances of an interpreter.(See [here](../../development/writing_zeppelin_interpreter.html) to understand more about its internal structure.) Zeppelin provides 3 different modes to run interpreter process: **shared**, **scoped** and **isolated**.   Also, the user can specify the scope of these modes: **per** user or **per note**.These 3 modes give flexibility to fit Zeppelin into any type of use cases.In this documentation, we mainly discuss the **per note** scope in combination with the **shared**, **scoped** and **isolated** modes.## Shared Mode    In **Shared** mode, single JVM process and a single session serves all notes. As a result, `note A` can access variables (e.g python, scala, ..) directly created from other notes.. ## Scoped Mode    In **Scoped** mode, Zeppelin still runs a single interpreter JVM process but, in the case of per note scope, each note runs in its own dedicated session. (Note it is still possible to share objects between these notes via [ResourcePool](../../interpreter/spark.html#object-exchange)) ## Isolated Mode    **Isolated** mode runs a separate interpreter process for each note in the case of **per note** scope. So, each note has an absolutely isolated session. (But it is still possible to share objects via [ResourcePool](../../interpreter/spark.html#object-exchange)) ## Which mode should I use?Mode | Each notebook...	| Benefits | Disadvantages | Sharing objects--- | --- | --- | --- | ---**shared** |  Shares a single session in a single interpreter process (JVM) |  Low resource utilization and it&#39;s easy to share data between notebooks |  All notebooks are affected if the interpreter process dies | Can share directly**scoped** | Has its own session in the same interpreter process (JVM) | Less resource utilization than isolated mode |  All notebooks are affected if the interpreter process dies | Can&#39;t share directly, but it&#39;s possible to share objects via [ResourcePool](../../interpreter/spark.html#object-exchange)) **isolated** | Has its own Interpreter Process | One notebook is not affected directly by other notebooks (**per note**) | Can&#39;t share data between notebooks easily (**per note**) | Can&#39;t share directly, but it&#39;s possible to share objects via [ResourcePool](../../interpreter/spark.html#object-exchange)) In the case of the **per user** scope (available in a multi-user environment), Zeppelin manages interpreter sessions on a per user basis rather than a per note basis. For example: - In **scoped + per user** mode, `User A`&#39;s notes **might** be affected by `User B`&#39;s notes. (e.g JVM dies, ...) Because all notes are running on the same JVM- On the other hand, **isolated + per user** mode, `User A`&#39;s notes will not be affected by others&#39; notes which running on separated JVMsEach Interpreter implementation may have different characteristics depending on the back end system that they integrate. And 3 interpreter modes can be used differently.Let’s take a look how Spark Interpreter implementation uses these 3 interpreter modes with **per note** scope, as an example.Spark Interpreter implementation includes 4 different interpreters in the group: Spark, SparkSQL, Pyspark and SparkR. SparkInterpreter instance embeds Scala REPL for interactive Spark API execution.    In Shared mode, a SparkContext and a Scala REPL is being shared among all interpreters in the group. So every note will be sharing single SparkContext and single Scala REPL. In this mode, if `Note A` defines variable ‘a’ then `Note B` not only able to read variable ‘a’ but also able to override the variable.    In Scoped mode, each note has its own Scala REPL. So variable defined in a note can not be read or overridden in another note. However, a single SparkContext still serves all the sessions.And all the jobs are submitted to this SparkContext and the fair scheduler schedules the jobs.This could be useful when user does not want to share Scala session, but want to keep single Spark application and leverage its fair scheduler.In Isolated mode, each note has its own SparkContext and Scala REPL.    ",
      "url": " /usage/interpreter/interpreter_binding_mode.html",
      "group": "usage/interpreter",
      "excerpt": ""
    }
    ,
    
  

    "/usage/interpreter/overview.html": {
      "title": "Interpreter in Apache Submarine",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Interpreter in Apache Submarine## OverviewIn this section, we will explain about the role of interpreters, interpreters group and interpreter settings in Zeppelin.The concept of Zeppelin interpreter allows any language/data-processing-backend to be plugged into Zeppelin.Currently, Zeppelin supports many interpreters such as Scala ( with Apache Spark ), Python ( with Apache Spark ), Spark SQL, JDBC, Markdown, Shell and so on.## What is Zeppelin interpreter?Zeppelin Interpreter is a plug-in which enables Zeppelin users to use a specific language/data-processing-backend. For example, to use Scala code in Zeppelin, you need `%spark` interpreter.When you click the ```+Create``` button in the interpreter page, the interpreter drop-down list box will show all the available interpreters on your server.## What is interpreter setting?Zeppelin interpreter setting is the configuration of a given interpreter on Zeppelin server. For example, the properties are required for hive JDBC interpreter to connect to the Hive server.Properties are exported as environment variables when property name is consisted of upper characters, numbers and underscore ([A-Z_0-9]). Otherwise set properties as JVM property. You may use parameters from the context of interpreter by add #{contextParameterName} in value, parameter can be of the following types: string, number, boolean.###### Context parameters      Name    Type        user    string        noteId    string        replName    string        className    string  If context parameter is null then replaced by empty string.Each notebook can be bound to multiple Interpreter Settings using setting icon on upper right corner of the notebook.## What is interpreter group?Every Interpreter is belonged to an **Interpreter Group**. Interpreter Group is a unit of start/stop interpreter.By default, every interpreter is belonged to a single group, but the group might contain more interpreters. For example, Spark interpreter group is including Spark support, pySpark, Spark SQL and the dependency loader.Technically, Zeppelin interpreters from the same group are running in the same JVM. For more information about this, please checkout [here](../development/writing_zeppelin_interpreter.html).Each interpreters is belonged to a single group and registered together. All of their properties are listed in the interpreter setting like below image.## Interpreter binding modeEach Interpreter Setting can choose one of &#39;shared&#39;, &#39;scoped&#39;, &#39;isolated&#39; interpreter binding mode.In &#39;shared&#39; mode, every notebook bound to the Interpreter Setting will share the single Interpreter instance. In &#39;scoped&#39; mode, each notebook will create new Interpreter instance in the same interpreter process. In &#39;isolated&#39; mode, each notebook will create new Interpreter process.For more information, check [Interpreter Binding Mode](./interpreter_binding_mode.html).## Connecting to the existing remote interpreterZeppelin users can start interpreter thread embedded in their service. This will provide flexibility to user to start interpreter on remote host. To start interpreter along with your service you have to create an instance of ``RemoteInterpreterServer`` and start it as follows:```javaRemoteInterpreterServer interpreter=new RemoteInterpreterServer(3678); // Here, 3678 is the port on which interpreter will listen.    interpreter.start();```The above code will start interpreter thread inside your process. Once the interpreter is started you can configure zeppelin to connect to RemoteInterpreter by checking **Connect to existing process** checkbox and then provide **Host** and **Port** on which interpreter process is listening as shown in the image below:## PrecodeSnippet of code (language of interpreter) that executes after initialization of the interpreter depends on [Binding mode](#interpreter-binding-mode). To configure add parameter with class of interpreter (`zeppelin..precode`) except JDBCInterpreter ([JDBC precode](../../interpreter/jdbc.html#usage-precode)).  ## Interpreter Lifecycle ManagementBefore 0.8.0, Zeppelin don&#39;t have lifecycle management on interpreter. User have to shutdown interpreters explicitly via UI. Starting from 0.8.0, Zeppelin provides a new interface`LifecycleManager` to control the lifecycle of interpreters. For now, there&#39;re 2 implementations: `NullLifecycleManager` and `TimeoutLifecycleManager` which is default. `NullLifecycleManager` will do nothing,user need to control the lifecycle of interpreter by themselves as before. `TimeoutLifecycleManager` will shutdown interpreters after interpreter idle for a while. By default, the idle threshold is 1 hour.User can change it via `zeppelin.interpreter.lifecyclemanager.timeout.threshold`. `TimeoutLifecycleManager` is the default lifecycle manager, user can change it via `zeppelin.interpreter.lifecyclemanager.class`.## Generic ConfInterpreterZeppelin&#39;s interpreter setting is shared by all users and notes, if you want to have different setting you have to create new interpreter, e.g. you can create `spark_jar1` for running spark with dependency jar1 and `spark_jar2` for running spark with dependency jar2.This approach works, but not so convenient. `ConfInterpreter` can provide more fine-grained control on interpreter setting and more flexibility. `ConfInterpreter` is a generic interpreter that could be used by any interpreters. The input format should be property file format.It can be used to make custom setting for any interpreter. But it requires to run before interpreter process launched. And when interpreter process is launched is determined by interpreter mode setting.So users needs to understand the ([interpreter mode setting ](../usage/interpreter/interpreter_bindings_mode.html) of Zeppelin and be aware when interpreter process is launched. E.g. If we set spark interpreter setting as isolated per note. Under this setting, each note will launch one interpreter process. In this scenario, user need to put `ConfInterpreter` as the first paragraph as the below example. Otherwise the customized setting can not be applied (Actually it would report ERROR)## Interpreter Process RecoveryBefore 0.8.0, shutting down Zeppelin also mean to shutdown all the running interpreter processes. Usually admin will shutdown Zeppelin server for maintenance or upgrade, but don&#39;t want to shut down the running interpreter processes.In such cases, interpreter process recovery is necessary. Starting from 0.8.0, user can enable interpreter process recovering via setting `zeppelin.recovery.storage.class` as `org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorage` or other implementations if available in future, by default it is `org.apache.zeppelin.interpreter.recovery.NullRecoveryStorage` which means recovery is not enabled. Enable recover means shutting down Zeppelin would not terminating interpreter process,and when Zeppelin is restarted, it would try to reconnect to the existing running interpreter processes. If you want to kill all the interpreter processes after terminating Zeppelin even when recovery is enabled, you can run `bin/stop-interpreter.sh` ",
      "url": " /usage/interpreter/overview.html",
      "group": "usage/interpreter",
      "excerpt": "This document explains about the role of interpreters, interpreters group and interpreter settings in Apache Submarine. The concept of Zeppelin interpreter allows any language/data-processing-backend to be plugged into Zeppelin."
    }
    ,
    
  

    "/usage/interpreter/user_impersonation.html": {
      "title": "Impersonation",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# ImpersonationUser impersonation enables to run zeppelin interpreter process as a web frontend user## Setup#### 1. Enable Shiro auth in `conf/shiro.ini````[users]user1 = password1, role1user2 = password2, role2```#### 2. Enable password-less ssh for the user you want to impersonate (say user1).```bashadduser user1#ssh-keygen (optional if you don&#39;t already have generated ssh-key.ssh user1@localhost mkdir -p .sshcat ~/.ssh/id_rsa.pub | ssh user1@localhost &#39;cat &gt;&gt; .ssh/authorized_keys&#39;```Alternatively instead of password-less, user can override ZEPPELIN_IMPERSONATE_CMD in zeppelin-env.sh```bashexport ZEPPELIN_IMPERSONATE_CMD=&#39;sudo -H -u ${ZEPPELIN_IMPERSONATE_USER} bash -c &#39;```#### 4. Restart zeppelin server.```bash# for OSX, linuxbin/zeppelin-daemon restart# for windowsbinzeppelin.cmd```#### 5. Configure impersonation for interpreter                         Go to interpreter setting page, and enable &quot;User Impersonate&quot; in any of the interpreter (in my example its shell interpreter)#### 6. Test with a simple paragraph```bash%shwhoami```Note that usage of &quot;User Impersonate&quot; option will enable Spark interpreter to use `--proxy-user` option with current user by default. If you want to disable `--proxy-user` option, then refer to `ZEPPELIN_IMPERSONATE_SPARK_PROXY_USER` variable in `conf/zeppelin-env.sh`",
      "url": " /usage/interpreter/user_impersonation.html",
      "group": "usage/interpreter",
      "excerpt": "Set up zeppelin interpreter process as web front end user."
    }
    ,
    
  

    "/usage/other_features/customizing_homepage.html": {
      "title": "Customizing Apache Submarine homepage",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Customizing Apache Submarine homepageApache Submarine allows you to use one of the notes you create as your Zeppelin Homepage.With that you can brand your Zeppelin installation, adjust the instruction to your users needs and even translate to other languages.## How to set a note as your Zeppelin homepageThe process for creating your homepage is very simple as shown below:1. Create a note using Zeppelin2. Set the note id in the config file3. Restart Zeppelin### Create a note using ZeppelinCreate a new note using Zeppelin,you can use ```%md``` interpreter for markdown content or any other interpreter you like.You can also use the display system to generate [text](../display_system/basic.html#text), [html](../display_system/basic.html#html), [table](../display_system/basic.html#table) orAngular ([backend API](../display_system/angular_backend.html), [frontend API](../display_system/angular_frontend.html)).Run (shift+Enter) the note and see the output. Optionally, change the note view to report to hidethe code sections.### Set the note id in the config fileTo set the note id in the config file, you should copy it from the last word in the note url.For example,Set the note id to the ```ZEPPELIN_NOTEBOOK_HOMESCREEN``` environment variableor ```zeppelin.notebook.homescreen``` property.You can also set the ```ZEPPELIN_NOTEBOOK_HOMESCREEN_HIDE``` environment variableor ```zeppelin.notebook.homescreen.hide``` property to hide the new note from the note list.### Restart ZeppelinRestart your Zeppelin server```bash./bin/zeppelin-daemon stop./bin/zeppelin-daemon start```That&#39;s it! Open your browser and navigate to Apache Submarine and see your customized homepage.## Show note list in your custom homepageIf you want to display the list of notes on your custom Apache Submarine homepage allyou need to do is use our %angular support.Add the following code to a paragraph in your Apache Submarine note and run it.```javascript%sparkprintln(&quot;&quot;&quot;%angular  &quot;&quot;&quot;)```After running the paragraph, you will see output similar to this one:That&#39;s it! Voila! You have your note list.",
      "url": " /usage/other_features/customizing_homepage.html",
      "group": "usage/other_features",
      "excerpt": "Apache Submarine allows you to use one of the notes you create as your Zeppelin Homepage. With that you can brand your Zeppelin installation, adjust the instruction to your users needs and even translate to other languages."
    }
    ,
    
  

    "/usage/other_features/personalized_mode.html": {
      "title": "Personalized Mode",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# What is Personalized Mode? ",
      "url": " /usage/other_features/personalized_mode.html",
      "group": "usage/other_features",
      "excerpt": ""
    }
    ,
    
  

    "/usage/other_features/publishing_paragraphs.html": {
      "title": "How can you publish your paragraphs",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# How can you publish your paragraphs?Apache Submarine provides a feature for publishing your notebook paragraph results. Using this feature, you can show Zeppelin notebook paragraph results in your own website. It&#39;s very straightforward. Just use `` tag in your page.## Copy a Paragraph LinkA first step to publish your paragraph result is **Copy a Paragraph Link**.  * After running a paragraph in your Zeppelin notebook, click a gear button located on the right side. Then, click **Link this Paragraph** menu like below image.    * Just copy the provided link. ## Embed the Paragraph to Your WebsiteFor publishing the copied paragraph, you may use `` tag in your website page.For example,```:/#/notebook/2B3QSZTKR/paragraph/...?asIframe&quot; height=&quot;&quot; width=&quot;&quot; &gt;```Finally, you can show off your beautiful visualization results in your website. &gt; **Note**: To embed the paragraph in a website, Apache Submarine needs to be reachable by that website. And please use this feature with caution and in a trusted environment only, as Zeppelin entire Webapp could be accessible by whoever visits your website. ",
      "url": " /usage/other_features/publishing_paragraphs.html",
      "group": "usage/other_features",
      "excerpt": "Apache Submarine provides a feature for publishing your notebook paragraph results. Using this feature, you can show Zeppelin notebook paragraph results in your own website."
    }
    ,
    
  

    "/usage/rest_api/configuration.html": {
      "title": "Apache Submarine Configuration REST API",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Apache Submarine Configuration REST API## OverviewApache Submarine provides several REST APIs for interaction and remote activation of zeppelin functionality.All REST APIs are available starting with the following endpoint `http://[zeppelin-server]:[zeppelin-port]/api`. Note that Apache Submarine REST APIs receive or return JSON objects, it is recommended for you to install some JSON viewers such as [JSONView](https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc).If you work with Apache Submarine and find a need for an additional REST API, please [file an issue or send us an email](http://submarine.apache.org/community.html).## Configuration REST API list### List all key/value pair of configurations                Description      This ```GET``` method return all key/value pair of configurations on the server.       Note: For security reason, some pairs would not be shown.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/configurations/all```              Success code      200               Fail code       500                sample JSON response                    {  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: {    &quot;zeppelin.war.tempdir&quot;: &quot;webapps&quot;,    &quot;zeppelin.notebook.homescreen.hide&quot;: &quot;false&quot;,    &quot;zeppelin.interpreter.remoterunner&quot;: &quot;bin/interpreter.sh&quot;,    &quot;zeppelin.notebook.s3.user&quot;: &quot;user&quot;,    &quot;zeppelin.server.port&quot;: &quot;8089&quot;,    &quot;zeppelin.dep.localrepo&quot;: &quot;local-repo&quot;,    &quot;zeppelin.ssl.truststore.type&quot;: &quot;JKS&quot;,    &quot;zeppelin.ssl.keystore.path&quot;: &quot;keystore&quot;,    &quot;zeppelin.notebook.s3.bucket&quot;: &quot;zeppelin&quot;,    &quot;zeppelin.server.addr&quot;: &quot;0.0.0.0&quot;,    &quot;zeppelin.ssl.client.auth&quot;: &quot;false&quot;,    &quot;zeppelin.server.context.path&quot;: &quot;/&quot;,    &quot;zeppelin.ssl.keystore.type&quot;: &quot;JKS&quot;,    &quot;zeppelin.ssl.truststore.path&quot;: &quot;truststore&quot;,    &quot;zeppelin.interpreters&quot;: &quot;org.apache.zeppelin.spark.SparkInterpreter,org.apache.zeppelin.spark.PySparkInterpreter,org.apache.zeppelin.spark.SparkRInterpreter,org.apache.zeppelin.spark.SparkSqlInterpreter,org.apache.zeppelin.spark.DepInterpreter,org.apache.zeppelin.markdown.Markdown,org.apache.zeppelin.angular.AngularInterpreter,org.apache.zeppelin.shell.ShellInterpreter,org.apache.zeppelin.flink.FlinkInterpreter,org.apache.zeppelin.lens.LensInterpreter,org.apache.zeppelin.ignite.IgniteInterpreter,org.apache.zeppelin.ignite.IgniteSqlInterpreter,org.apache.zeppelin.cassandra.CassandraInterpreter,org.apache.zeppelin.geode.GeodeOqlInterpreter,org.apache.zeppelin.kylin.KylinInterpreter,org.apache.zeppelin.elasticsearch.ElasticsearchInterpreter,org.apache.zeppelin.scalding.ScaldingInterpreter&quot;,    &quot;zeppelin.ssl&quot;: &quot;false&quot;,    &quot;zeppelin.notebook.autoInterpreterBinding&quot;: &quot;true&quot;,    &quot;zeppelin.notebook.homescreen&quot;: &quot;&quot;,    &quot;zeppelin.notebook.storage&quot;: &quot;org.apache.zeppelin.notebook.repo.VFSNotebookRepo&quot;,    &quot;zeppelin.interpreter.connect.timeout&quot;: &quot;30000&quot;,    &quot;zeppelin.anonymous.allowed&quot;: &quot;true&quot;,    &quot;zeppelin.server.allowed.origins&quot;:&quot;*&quot;,    &quot;zeppelin.encoding&quot;: &quot;UTF-8&quot;  }}      ### List all prefix matched key/value pair of configurations                Description      This ```GET``` method return all prefix matched key/value pair of configurations on the server.      Note: For security reason, some pairs would not be shown.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/configurations/prefix/[prefix]```              Success code      200               Fail code       500                sample JSON response            {  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: {    &quot;zeppelin.ssl.keystore.type&quot;: &quot;JKS&quot;,    &quot;zeppelin.ssl.truststore.path&quot;: &quot;truststore&quot;,    &quot;zeppelin.ssl.truststore.type&quot;: &quot;JKS&quot;,    &quot;zeppelin.ssl.keystore.path&quot;: &quot;keystore&quot;,    &quot;zeppelin.ssl&quot;: &quot;false&quot;,    &quot;zeppelin.ssl.client.auth&quot;: &quot;false&quot;  }}            ",
      "url": " /usage/rest_api/configuration.html",
      "group": "usage/rest_api",
      "excerpt": "This page contains Apache Submarine Configuration REST API information."
    }
    ,
    
  

    "/usage/rest_api/submarine_server.html": {
      "title": "Apache Submarine Server REST API",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Apache Submarine Server REST API## OverviewApache Submarine provides several REST APIs for interaction and remote activation of zeppelin functionality.All REST APIs are available starting with the following endpoint `http://[zeppelin-server]:[zeppelin-port]/api`. Note that Apache Submarine REST APIs receive or return JSON objects, it is recommended for you to install some JSON viewers such as [JSONView](https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc).If you work with Apache Submarine and find a need for an additional REST API, please [file an issue or send us an email](http://submarine.apache.org/community.html).## Zeppelin Server REST API list### Get Zeppelin version                Description      This ```GET``` method returns Zeppelin version              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/version```              Success code      200              Fail code      500              sample JSON response              {  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;Zeppelin version&quot;,  &quot;body&quot;: [    {      &quot;version&quot;: &quot;0.8.0&quot;,      &quot;git-commit-id&quot;: &quot;abc0123&quot;,      &quot;git-timestamp&quot;: &quot;2017-01-02 03:04:05&quot;    }  ]}                    ### Change the log level of Zeppelin Server                 Description      This ```PUT``` method is used to update the root logger&#39;s log level of the server.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/log/level/```              Success code      200              Fail code      406              sample JSON response              {  &quot;status&quot;: &quot;OK&quot;}                            sample error JSON response              {  &quot;status&quot;:&quot;NOT_ACCEPTABLE&quot;,  &quot;message&quot;:&quot;Please check LOG level specified. Valid values: DEBUG, ERROR, FATAL, INFO, TRACE, WARN&quot;}                    ",
      "url": " /usage/rest_api/submarine_server.html",
      "group": "usage/rest_api",
      "excerpt": "This page contains Apache Submarine Server REST API information."
    }
    
    
  
}
